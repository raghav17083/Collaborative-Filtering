While an inter-annotator agreement study is the best way to test the reliability of our two emotional annotations (FAH and CERT), our experience with annotating student emotions (Litman and Forbes-Riley, 2004) has shown that this type of annotation can be performed reliably. 
Although affect parameters have not been used in other PARADISE studies (to our knowledge), they are generic; for example, in various spoken dialogue systems, user affect has been annotated and automatically predicted from e.g., acoustic-prosodic and lexical features (Litman and Forbes-Riley, 2004b; Lee et al. , 2002; Ang et al. , 2002; Batliner et al. , 2003). 
In prior work we built on and generalized such research, by de ning a three-way distinction between negative, neutral, and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues (ForbesRiley and Litman, 2004; Litman and Forbes-Riley, 2004). 
We have developed an annotation scheme for hand labeling the student turns in our corpus with respect to three types of perceived emotions (Litman and Forbes-Riley, 2004): Negative: a strong expression of emotion such as confused, bored, frustrated, uncertain. 
Unlike our other parameters, these annotations are not currently available, although they can be predicted automatically (Litman and Forbes-Riley, 2004b), in our sys270 tem. 
