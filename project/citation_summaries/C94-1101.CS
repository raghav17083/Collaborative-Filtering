Nagao and Mori (1994) ran this procedure quite successfully on a large corpus of Japanese text. 
Noun phrase extraction(Nagao and Mori, 1994), word segmentation(Oda and Kita~ 579 1999) and diction extraction are the major issues. 
Recently, a new method which can calculate arbitrary number of n-gram statistics for very large corpora has been proposed (Nagao and Mori 1994). 
int suffix_compare(char **a, char **b){ return strcmp(*a, *b); } /* The input is a string, terminated with a null */ char **suffix_array(char *corpus){ int i, N = strten(corpus); char **result=(char **)rnalloc(N*sizeof(char *)); /* initialize result\[i\] with the ith suffix */ for(i=0; i < N; i++) result\[il = corpus + i; ClSOr't(result, N, sizeof(char *), suffix_compare); return result; } Nagao and Mori (1994) describe this procedure, and report that it works well on their corpus, and that it requires O(NlogN)time, assuming that the sort step requires O(NlogN) comparisons, and that each comparison requires 0(1) time. 
For Japanese, (Nagao and Mori, 1994) proposed a method of computing an arbitrary length character N-gram, and showed that the character N-gram statistics obtained from a large corpus includes information useful for word extraction. 
