The feature functions which are employed in the search process are:  Language model(s),  Direct and inverse IBM model-1,  Position-based word posterior probabilities (arc scores of the confusion network),  Word penalty,  N-gram frequencies (Chen et al., 2005),  N-gram posterior probabilities (Zens and Ney, 2006). 
We define the log-likelihood (LLH) of a target language sentence eI1 given a source language sentence fJ1 as: FMLN(M1,(eI1,fJ1 )) = Nsummationdisplay n=1 summationdisplay wn1eI1 logpM 1 (wn1|fJ1 ) Here, we use the n-gram posterior probability pM 1 (wn1|fJ1 ) as defined in (Zens and Ney, 2006). 
12Zens and Ney (2006) use a similar decision rule as here and they also use posterior n-gram probabilities as feature functions, but their model estimation and decoding are over an N-best, which is trivial in terms of computation. 
These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al. , 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the Nbest list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. 
We define NM 1 (wn1,fJ1 ) as in (Zens and Ney, 2006): NM 1 (wn1,fJ1 ) = summationdisplay I,eI1 In+1summationdisplay i=1 pM 1 (eI1|fJ1 )(ei+n1i,wn1) (4) The sum over the target language sentences is limited to an N-best list, i.e. the N best translation candidates according to the baseline model. 
