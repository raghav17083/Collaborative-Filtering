Familiar items present in training data can be listed in lexical resources, the probabilities of their different realizations can be estimated from corpus frequency distributions etc. Thus using lexical information (statistically extracted or handcrafted resources) is the most successful strategy in resolving syntactic ambiguities such as PP-attachment (Hindle and Rooth, 1993; Ratnaparkhi, 1998; Stetina and Nagao, 1997; Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), basing decisions on previous cases with identical lexemes or additional information about those lexemes.


Classifiers are commonly either supervised, with disambiguated training data, or more recently unsupervised (Ratnaparkhi, 1998) using data from unambiguous cases where no n1 or v appears.


Ratnaparkhi, 1998) and all occurrences of eat and related forms within ten words of with and with no intervening punctuation in the BNC were evaluated and tagged manually for this study.


PP-Attachment Accuracies of Previous Work method accuracy our method SVM 87.25% supervised Ratnaphakhi et al., 1994 ME 81.6% Brill and Resnik, 1994 TBL 81.9% Collins and Brooks, 1995 back-o 84.5% Zavrel et al., 1997 NN 84.4% Stetina and Nagao, 1997 DT 88.1% Abney et al., 1999 boosting 84.6% Vanschoenwinkel and Manderick, 2003 SVM 84.8% Zhao and Lin, 2004 NN 86.5% unsupervised Ratnaparkhi, 1998 81.9% Pantel and Lin, 2000 84.3% ME: Maximum Entropy, TBL: Transformation-Based Learning, DT: Decision Tree, NN: Nearest Neighbor configurations (McNemars test; p<0.05).


