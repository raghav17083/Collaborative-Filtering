4 Experimental Evaluation 4.1 Experiments Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a fiveor seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al., 2008). 
The latter is part of ongoing work motivated by the desire to both make use of corpus statistics and keep the advantage of the often (relative to automatic metricss scores) higher rank in human judgement given to rulebased systems on out-of-domain data, as seen on Figure 1: Translation with PBMT post-editing the WMT 2008 results for both English to French and German to English (Callison-Burch et al., 2008). 
Subsequently, in 2007 and 2008, the WMT Shared Translation Task organizers provided a limited amount of bilingual News Commentary training data (1-1.3M words) in addition to the large amount of Europarl data (30-32M words), and set up separate evaluations on News Commentary and on Europarl data, thus inviting interest in domain adaptation experiments for the News domain (Callison-Burch et al., 2007; Callison-Burch et al., 2008). 
For different models, the agreement rate varied from 67% to 78% (72% overall), and the Kappa value ranged from 0.34 to 0.55, which is comparable to figures reported for other standard SMT evaluation metrics (Callison-Burch et al., 2008). 
This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al., 2008). 
