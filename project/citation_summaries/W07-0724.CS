Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext LU+. 
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al., 2007b). 
In addition to taking geometric average, we also consider the arithmetic average: Na (s) := Nsummationdisplay n=1 wn |Xns | summationdisplay xXns P(x|U,n) P(x|L,n) (4) As a special case when N = 1, the score motivates selecting sentences which increase the number of unique words with new words appearing with higher frequency in U than L. 3.2 Similarity to the Bilingual Training Data (Similarity) The simplest way to expand the lexicon set is to choose sentences from U which are as dissimilar as possible to L. We measure the similarity using weighted n-gram coverage (Ueffing et al., 2007b). 
3.3 The Scoring Function In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (t,s) is scored according to the model probability p(t|s) normalized by the length|t|of the target sentence: Score(t,s) = p(t|s) 1|t| (3) Confidence Estimation: The confidence estimation which we implemented follows the approaches suggested in (Blatz et al. , 2003; Ueffing and Ney, 2007): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities, Levenshtein-based word posterior probabilities, and a target language model score. 
We plan to investigate  this  using the  PORTAGE  MT  system (Ueffing et al., 2007). 
