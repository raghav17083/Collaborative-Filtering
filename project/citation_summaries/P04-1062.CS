Smith and Eisner (2004) present an alternative estimation technique for CCM which uses annealing to try to escape local maxima. 
8 Future Work One weakness of all recent weighted grammar induction workincluding Klein and Manning (2004), Smith and Eisner (2005b), and the present paperis a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedulesnot to mention initialization. 
We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EMs criterion (Smith and Eisner, 2004). 
In future we might wish to apply techniques for avoiding local optima, such as deterministic annealing (Smith and Eisner, 2004). 
c2006 Association for Computational Linguistics Annealing Structural Bias in Multilingual Weighted Grammar Induction Noah A. Smith and Jason Eisner Department of Computer Science / Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD 21218 USA {nasmith,jason}@cs.jhu.edu Abstract We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). 
