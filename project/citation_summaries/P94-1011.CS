Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al. , 1995). 
It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. 
This method can be generalized, inspired by Stolcke and Segal (1994), who derive N-gram probabilities from stochastic context-free grammars. 
Another application in which prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (Stolcke and Segal 1994). 
Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al. , 1994). 
