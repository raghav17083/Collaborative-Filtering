Although affect parameters have not been used in other PARADISE studies (to our knowledge), they are generic; for example, in various spoken dialogue systems, user affect has been annotated and automatically predicted from e.g., acoustic-prosodic and lexical features (Litman and Forbes-Riley, 2004b; Lee et al. , 2002; Ang et al. , 2002; Batliner et al. , 2003). 
Speaker #Turns #Humor Male 685 347 (50.5% of Main) (50.6% of Male) Female 672 268 (49.5% Of Main) (39.9% of Female) Total 1357 615 Main (83.3% of Total) (86.1% of Humor) Table 2: Gender Distribution for Main Actors 210 5 Features Literature in emotional speech analysis (Liscombe et al. , 2003)(Litman and Forbes-Riley, 2004) (Scherer, 2003)(Ang et al. , 2002) has shown that prosodic features such as pitch, energy, speaking rate (tempo) are useful indicators of emotional states, such as joy, anger, fear, boredom etc. While humor is not necessarily considered as an emotional state, we noticed that most humorous utterances in our corpus (and also in general) often make use of hyper-articulations, similar to those found in emotional speech. 
There has been a considerable amount of research on incorporating affect (Litman and Forbes-Riley, 2004) (Alm et al. , 2005) (DMello et al. , 2005) (Shroder and Cowie, 2005) (Klein et al. , 2002) and personality (Gebhard et al. , 2004) in computer interfaces, so that, for instance, user frustrations can be recognized and addressed in a graceful manner. 
2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog (Batliner et al., 2004; Ang et al., 2002; Litman and Forbes-Riley, 2004; Rotaru et al., 2005). 
(Scherer, 2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthesis (TTS). 
