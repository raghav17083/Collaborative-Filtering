The improvement from training is consistent with the training results showed in (Xu et al. , 2002) where deleted-interpolation smoothing was used for the SLM components. 
For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1998; Xu et al. , 2002) uses syntactic structure to identify lexical items in the left-context which are then modeled as an n-gram process. 
Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al. , 2002; Xu et al. , 2003). 
The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations. 
A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al. , 2002; Charniak, 2001; Hall and Johnson, 2003). 
