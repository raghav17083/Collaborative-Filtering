It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). 
These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al. , 2005; Cahill and van Genabith, 2006). 
We have taken the Table-3 from (Guo et al., 2008), which shows the BLEU scores of different Paper BLEU score Langkilde(2002) 0.757 Nakanishi(2005) 0.705 Cahill(2006) 0.6651 Hogan(2007) 0.6882 White(2007) 0.5768 Guo(2008) 0.7440 Our Model 0.8156 Table 3: Comparsion of results for English WSJ section 23 systems on section 23 of PTB. 
More recently, statistical chart generators have emerged, including White (2004) for CCG, Carroll and Oepen (2005) and Nakanishi et al. 
Both metrics have recently been used to evaluate generators (Langkilde-Geary, 2002; Nakanishi et al. , 2005; Belz and Reiter, 2006). 
