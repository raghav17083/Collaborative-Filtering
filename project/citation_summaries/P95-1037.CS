We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magermans rules (1995). 
To make the model more practical in parameter estimation, we assume the features in feature set FS are independent from each other, thus:   = FSFi AFiPAFSP ),|(),|(  (5) Under this PCFG+PF model, the goal of a parser is to choose a parse that maximizes the following score: )|,(maxarg)|( 1 AFS i i i n i T PSTScore  = = (6) Our model is thus a simplification of more sophisticated models which integrate PCFGs with features, such as those in Magerman(1995), Collins(1997) and Goodman(1997). 
The total word accuracy on Wall St. Journal data, 96.5%(Magerman, 1995), is similar to that presented in this paper. 
Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models (Black et al. , 1993; Magerman, 1995), training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al. , 1993), or other techniques (Bod, 1993). 
In retrospect, however, there are perhaps even greater similarities to that of (Magerman, 1995; Henderson, 2003; Matsuzaki et al. , 2005). 
