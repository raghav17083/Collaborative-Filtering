P07-1007:1	36:190	2.1 Choice of Corpus The DSO corpus (Ng and Lee, 1996) contains 192,800 annotated examples for 121 nouns and 70 verbs, drawn from BC and WSJ.
---------------------------------------------------
W09-2420:2	26:122	The most commonly used dataset is the Defense Science Organization (DSO) corpus (Ng and Lee, 1996), which comprises sentences from two different corpora.
---------------------------------------------------
W09-2420:3	11:122	Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005).
---------------------------------------------------
N09-1004:4	25:224	In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996).
---------------------------------------------------
P98-2228:5	27:113	In this work, we shall adopt the methodology first explicitly noted in connection with WSD by (McRoy, 1992), and more recently (Ng and Lee, 1996), namely that of bringing together a number of partial sources of information about a phenomenon and combining them in a principled manner.
---------------------------------------------------
J01-3001:6	119:520	This consisted of 192,800 occurrences of the 121 nouns and 70 verbs that are "the most frequently occurring and ambiguous words in English" (Ng and Lee 1996, 44).
---------------------------------------------------
J01-3001:7	134:520	Both Ng and Lee (1996) and Yarowsky (1993) reported some results in the area.
---------------------------------------------------
J01-3001:8	103:520	Ng and Lee (1996) explored an approach to WSD in which a word is assigned the sense of the most similar example already seen.
---------------------------------------------------
W04-0834:9	33:87	Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al. , 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy.
---------------------------------------------------
W04-0834:10	47:87	To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: a1 a1a45a12a46a41a44a1a45a12, a1 a12a46a41a44a12, a1 a1a10a9a7a41a44a1a10a9, a1 a9a7a41a9, a1 a1a10a9a7a41a44a1a13a12, a1 a1a13a12a36a41a44a12, a1 a12a36a41a9, a1 a1a4a3a47a41a44a1a45a12, a1 a1a48a9a18a41a44a12, a1 a1a13a12a36a41a9, and a1 a12a36a41a3 . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).
---------------------------------------------------
W04-0827:11	57:122	The method we use to extract the keywords for each sense is based on the work of Ng and Lee (1996).
---------------------------------------------------
W04-0827:12	18:122	Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al.
---------------------------------------------------
W02-0812:13	141:151	Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of cooccurrence features the accuracy only drops to 80%.
---------------------------------------------------
P07-1006:14	34:188	2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schtze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001).
---------------------------------------------------
W99-0622:15	123:280	Ng and Lee (1996) report results on disambiguat 6The improvement gotten for moving from binary to rt-ary relations, when using WordNet, is not significant.
---------------------------------------------------
W99-0622:16	129:280	Since the most frequent words are typically the most polysemous, the ambiguity problem is more severe for this subset, but there is also more data: we have about 24,000 instances of 10,000 distinct senses in our corpus, and Ng and Lee (1996) use 192,800 occurrences of their 191 words.
---------------------------------------------------
P06-1134:17	151:194	This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of SENSEVAL systems.
---------------------------------------------------
W02-1006:18	54:163	This parameter is also used by (Ng and Lee, 1996).
---------------------------------------------------
W02-1006:19	59:163	To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: a52 a4a14a13a59a53a58a4a14a13, a52 a13a59a53a58a13, a52 a4a7a12a57a53a58a4a7a12, a52 a12a57a53a12, a52 a4a7a12a57a53a58a4a14a13, a52 a4a14a13a59a53a58a13, a52 a13a59a53a12, a52 a4a7a6a57a53a58a4a14a13, a52 a4a7a12a57a53a58a13, a52 a4a14a13a59a53a12, and a52 a13a59a53a6 . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).
---------------------------------------------------
W02-1006:20	33:163	Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word interest.
---------------------------------------------------
W02-1006:21	148:163	tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996).
---------------------------------------------------
N07-1025:22	8:153	Among the various knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance.
---------------------------------------------------
N07-1025:23	82:153	This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of state-ofthe-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3 evaluations.
---------------------------------------------------
P97-1009:24	7:195	In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus.
---------------------------------------------------
W04-2404:25	214:254	The LEXAS system of (Ng and Lee, 1996) uses part of speech, morphology, co-occurrences, collocations and verb object relation in nearest neighbor implementation.
---------------------------------------------------
W04-2404:26	15:254	Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g. , (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)).
---------------------------------------------------
W00-0706:27	57:128	3 Setting A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO corpus, which is a semantically annotated English corpus collected by Ng and colleagues (Ng and Lee, 1996).
---------------------------------------------------
W00-0706:28	68:128	Accuracy 2This set of attributes corresponds to that used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation.
---------------------------------------------------
W97-0108:29	49:196	Corpus-based word sense disambignation algorjthm~ such as (Ng and Lee, 1996; Bruce and Wiebe, 1994; Yarowsky, 1994) relied on supervised learning fzom annotated corpora.
---------------------------------------------------
W97-0108:30	191:196	Ng and Lee (1996) found that train/rig sets of 1000-1500 e~mples per word are necessary for sense dJ-~mhiguation of one highly ambiguous word.
---------------------------------------------------
W97-0108:31	53:196	,1994) and the DSO corpus of 192,800 sense-tagged occtuTences of 191 words used by (Ng and Lee, 1996) are still insu~cient in scale for supervised algorithms to perform well on a wide range of texts.
---------------------------------------------------
W97-0108:32	165:196	The first is the LBXAS algorithm which uses an exemplar-based learning framework s;mil~to the case-based reasoning foundation of Kenmore (Ng, 1997; Ng and Lee, 1996).
---------------------------------------------------
W02-0813:33	20:111	Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information.
---------------------------------------------------
J98-1003:34	14:551	WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schi.itze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996).
---------------------------------------------------
W02-0817:35	30:149	The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora.
---------------------------------------------------
W07-2054:36	39:92	2.3 DSO Corpus Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD.
---------------------------------------------------
W07-2054:37	15:92	Hence, besides gathering examples from the widely used SEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996).
---------------------------------------------------
J04-1001:38	41:334	They include those using naive Bayes (Gale, Church, and Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996), transformation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000).
---------------------------------------------------
C02-1127:39	17:203	Knowledge-based work, such as (Hirst, 1987; McRoy, 1992; Ng and Lee, 1996) used hand-coded rules or supervised machine learning based on annotated corpus to perform WSD.
---------------------------------------------------
P99-1020:40	10:160	WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al. , 1992), (Ng and Lee, 1996); 3.
---------------------------------------------------
P99-1020:41	12:160	There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al. , 1997).
---------------------------------------------------
W97-0201:42	33:114	This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996).
---------------------------------------------------
W97-0201:43	72:114	The performance figures of LEXAS in Table 1 are higher than those reported in (Ng and Lee, 1996).
---------------------------------------------------
W97-0201:44	54:114	2 To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplarbased WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in (Ng and Lee, 1996)).
---------------------------------------------------
W97-0201:45	3:114	Using the sense-tagged corpus of 192,800 word occurrences reported in (Ng and Lee, 1996), I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier.
---------------------------------------------------
W97-0201:46	52:114	To overcome this data sparseness problem of WSD, I initiated a mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have been manually tagged with senses of WORDNET (Ng and Lee, 1996).
---------------------------------------------------
W97-0201:47	16:114	In Section 3, I examine the size of the training corpus on the accuracy of WSD, using a corpus of 192,800 occurrences of 191 words hand tagged with WORDNET senses (Ng and Lee, 1996).
---------------------------------------------------
W97-0201:48	28:114	When the task is to resolve word senses to the fine-grain distinction of WORDNET senses, the accuracy figures achieved are generally not very high (Miller et al. , 1994; Ng and Lee, 1996).
---------------------------------------------------
W97-0201:49	69:114	The two test sets, BC50 and WSJ6, are the same as those reported in (Ng and Lee, 1996).
---------------------------------------------------
P97-1067:50	14:71	2 Generalizing Lexical Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowsky, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996).
---------------------------------------------------
W01-0704:51	42:133	For training, DSO sense tagged English corpus (Hwee Tou Ng and Hian Beng Lee, 1996) is used.
---------------------------------------------------
W97-0321:52	15:160	Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996).
---------------------------------------------------
W97-0321:53	8:160	Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, e.g., machine translation and information retrieving (Allen, 1995; Ng and Lee, 1996; Resnik, 1995), etc. Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method.
---------------------------------------------------
W97-0321:54	12:160	Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verbnoun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996).
---------------------------------------------------
C04-1131:55	79:144	Many studies do not consider all the words of the context (El-Bze, Loupy, Marteau, 1998; Mooney, 1996; Ng, Lee, 2002).
---------------------------------------------------
C04-1131:56	18:144	For instance, by using these dictionaries, the inter-annotator agreement may sometimes reach only 57% (Ng, Lee, 1996) or may simply be equivalent to random sense allocation (Vronis, 1998).
---------------------------------------------------
A97-2010:57	1:30	A Broad-Coverage Word Sense Tagger Dekang Lin Department of Computer Science University of Manitoba Winnipeg, Manitoba, Canada R3T 2N2 lindek@cs.umanitoba.ca Previous corpus-based Word Sense Disambiguation (WSD) algorithms (Hearst, 1991; Bruce and Wiebe, 1994; Leacock et al. , 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1995) determine the meanings of polysemous words by exploiting their local contexts.
---------------------------------------------------
W97-0202:58	10:153	The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al. 1994, Ng & Hian 1996, and Grishman et al. 1994) and for performing disambiguation (Resnik 1995 and Leacock et ai.
---------------------------------------------------
W04-0847:59	8:69	Bayesian learning (Leacock et al. , 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al. , 2002), etc In this paper, we employ Naive Bayes classifier to perform WSD.
---------------------------------------------------
D09-1145:60	106:257	For example, machine learning methods with features based on part-of-speech tags, word stems, surrounding and co-occurring words, and dependency relationships have been successfully used in WSD (Montoyo et al., 2005; Ng and Lee, 1996; Dligach and Palmer, 2008) and positional features such as the position of a sentence in the document have been used in text summarization (e.g.
---------------------------------------------------
W00-1326:61	72:234	These collocations have been used in other word sense disambiguation research and are also referred to as features (Gale et al. , 1993; Ng & Lee, 1996; Escudero et al. , 2000).
---------------------------------------------------
W00-1326:62	37:234	1 Resources used The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word.
---------------------------------------------------
W00-1326:63	15:234	Since the papers were published, word sense disambiguation has moved to deal with finegrained sense distinctions from widely recognized semantic lexical resources; ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, etc. or machine-readable dictionaries like OALDC, Webster's, LDOCE, etc. This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al. , 1993) and the DSO collection (Ng & Lee, 1996).
---------------------------------------------------
C08-1143:64	120:207	The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996).
---------------------------------------------------
P06-1012:65	81:170	4.1 DSO Corpus The DSO corpus (Ng and Lee, 1996) contains 192,800 annotated examples for 121 nouns and 70 verbs, drawn from BC and WSJ.
---------------------------------------------------
W97-0322:66	194:250	203 Discovered Actual worry business worry 166 281 447 business 181 607 788 347 888 1235 McQuitty 773 correct Discovered Actual worry business worry 288 159 447 business 155 633 788 443 792 1235 Ward921 correct Actual worry business Discovered worry business 384 63 132 656 516 719 447 788 1235 EM 1040 correct Figure 6: concern Feature Set A Discovered Actual attention share money attention 53 6 302 361 share 58 187 255 500 money 108 4 1140 1252 219 197 1697 2113 McQuitty 1380 correct Discovered Actual attention share money attention 280 3 78 361 share 240 197 63 500 money 559 0 693 1252 1079 200 834 2113 Ward 1170 correct Discovered Actual attention share money attention 127 230 4 361 share 134 364 2 500 money 320 124 808 1252 581 718 814 2113 EM 1299 correct Figure 7: interest Feature Set B Discovered Actual assist enhance assist 45 234 279 enhance 146 842 988 191 1076 1267 McQuitty 887 correct Actual assist enhance Discovered assist enhance 88 191 354 634 442 825 279 988 1267 Ward 722 correct Actual assist enhance Discovered assist enhance 119 160 279 344 644 988 463 804 1267 EM 763 correct Figure 8: help Feature Set C 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g. , (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
---------------------------------------------------
W98-0703:67	13:142	There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al. , 1997).
---------------------------------------------------
W98-0703:68	11:142	WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al. , 1992), (Ng and Lee, 1996}; 3.
---------------------------------------------------
W00-1202:69	9:216	In English, only some sense-tagged corpora such as HECTOR (Atkins, 1993), DSO (Ng and Lee, 1996), SEMCOR (Fellbaum, 1997), and SENSEVAL (Kilgarriff, 1998) are available.
---------------------------------------------------
W07-2047:70	16:85	The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996).
---------------------------------------------------
E99-1028:71	6:132	1 Introduction One of the major approaches to disambiguate word senses is supervised learning (Gale et al. , 1992), (Yarowsky, 1992), (Bruce and Janyce, 1994), (Miller et al. , 1994), (Niwa and Nitta, 1994), (Luk, 1995), (Ng and Lee, 1996), (Wilks and Stevenson, 1998).
---------------------------------------------------
W97-0213:72	59:184	In contrast, approaches to WSD attempt to take advantage of many different sources of information (e.g. see (McRoy, 1992; Ng and Lee, 1996; Bruce and Wiebe, 1994)); it seems possible to obtain benefit from sources ranging from local collocational clues (Yarowsky, 1993) to membership in semantically or topically related word classes (arowsky, 1992; Resnik, 1993) to consistency of word usages within a discourse (Gale et al. , 1992); and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word.
---------------------------------------------------
J03-4004:73	194:239	Although a modest amount of sense-tagged data is available for English (Miller et al. 1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, the heuristic is not applicable.
---------------------------------------------------
W97-0323:74	104:132	Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of (Ng and Lee, 1996) in the third row, also with k = 1.
---------------------------------------------------
W97-0323:75	99:132	However, the feature value pruning method of (Ng and Lee, 1996) only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See (Ng and Lee, 1996) for details).
---------------------------------------------------
W97-0323:76	70:132	This corpus was first reported in (Ng and Lee, 1996), and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English.
---------------------------------------------------
W97-0323:77	97:132	The accuracy figures of LEXAS as reported in (Ng and Lee, 1996) are reproduced in the third row of Table 1.
---------------------------------------------------
W97-0323:78	103:132	However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:79	123:132	Our past work (Ng and Lee, 1996) suggests that multiple sources of knowledge are indeed useful for WSD.
---------------------------------------------------
W97-0323:80	52:132	, k = 1, no exemplar weighting, no feature weighting, etc. We have used the default values for all parameter settings in our previous work on exemplar-based WSD reported in (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:81	6:132	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line".
---------------------------------------------------
W97-0323:82	87:132	2The first five of these seven features were also used in (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:83	12:132	On the other hand, our past work on WSD (Ng and Lee, 1996) used an exemplar-based (or nearest neighbor) learning approach.
---------------------------------------------------
W97-0323:84	88:132	210 Algorithm BC50 WSJ6 Sense 1 40.5% 44.8% Most Frequent 47.1% 63.7% Ng & Lee (1996) 54.0% 68.6% PEBLS (k = 1) 55.0% 70.2% PEBLS (k = 20) 58.5% 74.5% PEBLS (10-fold c.v).
---------------------------------------------------
W97-0323:85	19:132	By using 10-fold cross validation (Kohavi and 208 John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:86	24:132	Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:87	105:132	The feature value pruning method of (Ng and Lee, 1996) is intended to keep only feature values deemed important for classification.
---------------------------------------------------
W97-0323:88	127:132	Also, given the relative importance of the various knowledge sources as reported in (Ng and Lee, 1996), it may be possible to improve disambignation performance by introducing feature weighting.
---------------------------------------------------
W97-0323:89	3:132	By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:90	77:132	Both test sets are identical to the ones reported in (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:91	79:132	Local collocations have been found to be the single most informative set of features for WSD (Ng and Lee, 1996).
---------------------------------------------------
W97-0323:92	89:132	58.7% 75.2% Naive-Bayes 58.2% 74.5% Table 1: Experimental Results ures are those of (Ng and Lee, 1996).
---------------------------------------------------
W97-0212:93	90:101	Ng and Lee (1996) have found only 57% agreement when comparing the same texts tagged according to the same dictionary senses by different (human)!
---------------------------------------------------
W04-0839:94	70:101	Further, some of the work (e.g. , (McRoy, 1992), (Ng and Lee, 1996)) suggests that using both kinds of features may result in significantly higher accuracies as compared to individual results.
---------------------------------------------------
W04-0839:95	8:101	Prior research (e.g. , (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies.
---------------------------------------------------
A00-2009:96	55:165	This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al. , 1997), and (Pedersen and Bruce, 1997).
---------------------------------------------------
A00-2009:97	122:165	A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g. , (Leacock et al. , 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)).
---------------------------------------------------
A00-2009:98	81:165	For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data.
---------------------------------------------------
A00-2009:99	10:165	Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g. , (Leacock et al. , 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealersen and Bruce, 1997)).
---------------------------------------------------
A00-2009:100	13:165	(Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.
---------------------------------------------------
A00-2009:101	130:165	For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%.
---------------------------------------------------
A00-2009:102	93:165	and Lee, 1996), who represent the context of an ambiguous word with the part-of-speech of three words to the left and right of interest, a morphological feature indicating if interest is singular or plural, an unordered set of frequently occurring keywords that surround interest, local collocations that include interest, and verb-object syntactic relationships.
---------------------------------------------------
W04-0209:103	40:186	Another corpus with WordNet-based semantic annotation is the DSO corpus (Ng and Lee, 1996).
---------------------------------------------------
A97-1056:104	94:185	2An alternative feature set for this data is utilized with an exemplar-based learning algorithm in (Ng and Lee, 1996).
---------------------------------------------------
P99-1004:105	46:249	Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities.
---------------------------------------------------
E99-1046:106	9:39	Even worse news comes from (Ng and Lee, 1996), who re-tagged parts of the manually tagged SEMCOR corpus (Fellbaum, 1998).
---------------------------------------------------
W97-0811:107	8:94	(See \[Dagan and Itai, 1994; Gale et al. , 1992a; Gale et al, 1992b; Ng and Lee, 1996; Wilks, 1996; Yarowski, 1992; Yarowski, 1995\] for recent work on word sense disambiguation).
---------------------------------------------------
W00-1322:108	76:173	This corpus was collected by Ng and colleagues (Ng and Lee, 1996) and it is available from the Linguistic Data Consortium (LDC) 5.
---------------------------------------------------
W00-1322:109	91:173	7The already described set of attributes contains those attributes used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation.
---------------------------------------------------
W00-1322:110	20:173	172 one hand, WSD is very dependant to the domain of application (Gale et al. , 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora.
---------------------------------------------------
W00-1322:111	123:173	SThe second and third column correspond to the train and test sets used by (Ng and Lee, 1996; Ng, 1997a)  Regarding the portability of the systems, very disappointing results are obtained.
---------------------------------------------------
W02-0809:112	79:137	The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates.
---------------------------------------------------
W02-0809:113	28:137	Our approach to memory-based all-words WSD follows the memorybased approach of (Ng and Lee, 1996), and the work by (Veenstra et al. , 2000) on a memorybased approach to the English lexical sample task of SENSEVAL-1.
---------------------------------------------------
W02-0814:114	14:172	Recently (both SENSEVAL competitions), various machine learning (ML) approaches have been demonstrated to produce relatively successful WSD systems, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al. , 2000), decision lists (Yarowsky, 2000), boosting (Escudero et al. , 2000).
---------------------------------------------------
W02-0814:115	68:172	The method used to extract these keywords for each sense is based on the work of Ng and Lee (1996).
---------------------------------------------------
W03-0106:116	16:130	Knowledge-based work, such as [Hirst 1987; McRoy 1992; Ng and Lee 1996] used hand-coded rules or supervised machine learning based on an annotated corpus to perform WSD.
---------------------------------------------------
W99-0502:117	4:9	It us widely acknowledged that word sense d~samblguatmn (WSD) us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching, the problem of d~samblguatmg word sense, or dlscermng the meamng of a word m context, must be effectively dealt with Advances in WSD v, ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s)ntactm parsing, there are relatlvely well defined and agreed-upon cnterm of what it means to have the "correct" part of speech or syntactic structure assigned to a word or sentence For instance, the Penn Treebank corpus (Marcus et al, 1993) pro~ide~,t large repo.~tory of texts annotated w~th partof-speech and s}ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g~ven sentence Unfortunately, th~s us not the case for word sense assignment F~rstly, it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a g~ven word Different d~ctlonanes tend to carve up the "semantic space" m a different way, so to speak Secondly, the hst of senses for a word m a typical dmtmnar~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d~ctmnary hke WoRDNET (Miller, 1990) tend to be rather fine Hence, two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance, the SENSEVAL exerclse has performed a detaded study to find out the raterannotator agreement among ~ts lexicographers taggrog the word senses (Kllgamff, 1998c, Kllgarnff, 1998a, Kflgarrlff, 1998b) 2 A Case Study In this-paper, we examine the ~ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus (Miller et al, 1993) and the DSO corpus (Ng and Lee, 1996, Ng, 1997), which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~VoRDNET senses, and consists of more than 670,000 words from 352 text files Sense taggmg was done on the content words (nouns, ~erbs, adjectives and adverbs) m this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a hst of 191 frequently occurring words of Enghsh (121 nouns and 70 verbs), sentences containing w (m singular or plural form, and m its various reflectional verb form) are selected and each word occurrence w ~s tagged w~th a sense from WoRDNET There ~s a total of about 192,800 sentences in the DSO corpus m which one word occurrence has been sense-tagged m each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences m which a word occurrence w is sense-tagged m each sentence, where w Is one of.the 191 frequently oc-,currmg English nouns or verbs Since this common pomon has been sense-tagged by two independent groups of human annotators, ~t serves as our data set for investigating inter-annotator agreement in this paper 3 Sentence Matching To determine the extent of inter-annotator agreement, the first step ~s to match each sentence m Semcor to its corresponding counterpart In the DSO corpus This step ~s comphcated by the following factors 1 Although the intersected portion of both corpora came from Brown corpus, they adopted different tokemzatmn convention, and segmentartan into sentences differed sometimes 2 The latest versmn of Semcor makes use of the senses from WORDNET 1 6, whereas the senses used m the DSO corpus were from WoRDNET 15 1 To match the sentences, we first converted the senses m the DSO corpus to those of WORDNET 1 6 We ignored all sentences m the DSO corpus m which a word is tagged with sense 0 or -1 (A word is tagged with sense 0 or -1 ff none of the given senses m WoRDNFT applies ) 4, sentence from Semcor is considered to match one from the DSO corpus ff both sentences are exactl) ldent~cal or ff the~ differ only m the pre~ence or absence of the characters " (permd) or -' (hyphen) For each remaining Semcor sentence, taking into account word ordering, ff 75% or more of the words m the sentence match those in a DSO corpus sentence, then a potential match ~s recorded These i -kctua\[ly, the WORD~q'ET senses used m the DSO corpus were from a shght variant of the official WORDNE'I 1 5 release Th~s ssas brought to our attention after the pubhc release of the DSO corpus potential matches are then manually verffied to ensure that they are true matches and to ~eed out any false matches Using this method of matching, a total of 13,188 sentence-palrs contasnmg nouns and 17,127 sentence-pa~rs containing verbs are found to match from both corpora, ymldmg 30,315 sentences which form the intersected corpus used m our present study 4 The Kappa Statistic Suppose there are N sentences m our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b~ two human annotators Then a simple measure to quantify the agreement rate between two human annotators Is Pc, where Pc, = A/N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a (Cohen, 1960) is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w~thm computatmnal hngu~stlcs to measure raterannotator agreement (Bruce and Wmbe, 1998, Carletta, 1996, Veroms, 1998) Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P~-P~ 1-P~ where M j=l and Pe measures the chance agreement between two annotators A Kappa ~alue of 0 indicates that the agreement is purely due to chance agreement, whereas a Kappa ~alue of 1 indicates perfect agreement A Kappa ~alue of 0 8 and above is considered as mdmatmg good agreement (Carletta, 1996) Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first (becond) row denotes agreement on the nouns (xerbs), wh~le the lass row denotes agreement on all words combined The a~erage ~ reported m the table is a s~mpie average of the individual ~ value of each word The agreement rate on the 30,315 sentences as measured by P= is 57% This tallies with the figure reported ~n our earlier paper (Ng and Lee, 1996) where we performed a quick test on a subset of 5,317 sentences,n the intersection of both the Semcor corpus and the DSO corpus 10 \[\] mm m m m m m mm m m m m mm m m m Type Num of v, ords A N \[ P~ Avg Nouns 121 7,676 13,188 I 0 582 0 300 Verbs 70 9,520 17,127 I 0 555 0 347 All I 191 I 17,196 30,315 I 056T 0317 Table 1 Raw inter-annotator agreement 5 Algorithm Since the rater-annotator agreement on the intersected corpus is not high, we would like to find out how the agreement rate would be affected if different sense classes were in use In this section, we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given m Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sense-tagged by two human annotators At each Iteration of the algorithm, tt finds the pair of sense classes Ct and Cj such that merging these two sense classes results in the highest t~ value for the resulting merged group of sense classes It then proceeds to merge Cz and C~ Thin process Is repeated until the ~ value reaches a satisfactory value ~,~t,~, which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging, speech-act categorization, etc 6 Results For each word w from the list of 121 nouns and 70 verbs, ~e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w For a subset of 95 words (53 nouns and 42 verbs), the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa ~alue reaches 0 8 or higher For the other 96 words, m order for the Kappa value to reach 0 8 or higher, the algorithm collapses all senses of the ~ord to a single (trivial) class Table 2 and 3 summarizes the results for the set of 53 nouns and 42 ~erbs, respectively Table 2 md~cates that before the collapse of sense classes, these 53 nouns have an average of 7 6 senses per noun There is a total of 5,339 sentences in the intersected corpus containing these nouns, of which 3,387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic (computed as a simple average of the Kappa statistic of ~he mdlwdual nouns) is 0 463 After the collapse of sense classes by the greedy search algorithm, the average number of senses per noun for these 53 nouns drops to 40 Howe~er, the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5,033 That is, about 94 3% of the sentences have been assigned the same coarse sense, and that the average Kappa statistic has improved to 0 862, mgmfymg high rater-annotator agreement on the derived coarse senses Table3 gl~es the analogous figures for the 42 verbs, agmn mdmatmg that high agreement is achieved on the coarse sense classes den~ed for verbs 7 Discussion Our findings on rater-annotator agreement for word sense tagging indicate that for average language users, it is quite dl~cult to achieve high agreement when they are asked to assign refned sense tags (such as those found in WORDNET) given only the scanty definition entries m the WORDNET dlctionary and a few or no example sentences for the usage of each word sense Thin observation agrees wlth that obtmned m a recent study done by (Veroms, 1998), where the agreement on sense-tagging by naive users was also not hlgh Thus It appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very fine-grained resolutmn as formulated m a tradltlonal dmtlonary In contrast, expert lexicographers tagged the ~ ord sense in the sentences used m the SENSEVAL exerclse, where high rater-annotator agreement was reported There are also fuller dlctlonary entries m the HECTOR dlctlonary used and more e<amples showing the usage of each word sense m HECTOR These factors are likely to have contributed to the difference in rater-annotator agreement observed m the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm Vv'e found some interesting groupings of coarse senses for nouns which ~e hst in Table 4 From Table 4, it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense graz}.ulanty It Is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example, there is a total Ii loop: let Ct,, C M denote the current M sense classes ~* +--oo for all z,3 such that 1 <, < 3 < M let C\[,,C~w_ 1 denote the resulting M 1 sense classes by mergmg C, and C 3 compute ~(C\[,, C~/_t) ff ~(C,, C~4_x) > ~* then ~" +~(C~,,C~_t), z* +~, ~* +end for merge the sense class C,.
---------------------------------------------------
P02-1044:118	17:149	They include those using Nave Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs.
---------------------------------------------------
N01-1010:119	33:205	We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al. , 1998) and DSO (Ng and Lee, 1996).
---------------------------------------------------
N01-1010:120	148:205	6 Evaluation: Inter-annotator Disagreement To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al. , 1998) and DSO (Ng and Lee, 1996).
---------------------------------------------------
J98-4002:121	11:389	Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995).
---------------------------------------------------
W98-1126:122	115:229	4.1.1 Per-Class Method In the per-class method (also used by Ng and Lee 1996), a set of words, WordsCiSj, is selected for each combination of Class Ci and subproperty Sj.
---------------------------------------------------
W98-1126:123	68:229	For example, the constraint might be that the word must appear immediately to the right of the target word (see, for example, Ng & Lee 1996 and Bruce & Wiebe 1994); the actual collocations would be words that occur there.
---------------------------------------------------
P97-1056:124	56:175	More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins ~z Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng & Lee, 1996; Collins, 1996).
---------------------------------------------------
