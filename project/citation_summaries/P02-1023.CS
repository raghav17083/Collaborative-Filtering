Gao and Zhang (2002) also presented a method of combining two criteria, and showed the combination of rank and entropy achieved the smallest models.


A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002).


In Gao and Zhang (2002), three measures are studied for the purpose of language model pruning.


Efforts targeting ef ciency have been mainly focused on pruning techniques (Seymore and Rosenfeld, 1996; Gao and Zhang, 2002), which permit to signi cantly reduce the amount of n-grams to be stored at a negligible cost in performance.


