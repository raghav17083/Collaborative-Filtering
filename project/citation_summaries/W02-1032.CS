H05-1027:1	176:258	Notice that the result is much better than the state-of-the-art performance currently available in the marketplace (e.g. Gao et al. 2002), presumably due to the large amount of training data we used, and to the similarity between the training and the test data.
---------------------------------------------------
H05-1027:2	168:258	For each A, we produced a word lattice using the baseline system described in (Gao et al. 2002), which uses a word trigram model trained via MLE on anther 400,000-sentence subset of the Nikkei Newspaper corpus.
---------------------------------------------------
H05-1027:3	7:258	1 Introduction Language modeling (LM) is fundamental to a wide range of applications, such as speech recognition and Asian language text input (Jelinek 1997; Gao et al. 2002).
---------------------------------------------------
H05-1027:4	29:258	Current IME systems make about 5-15% CER in conversion of real data in a wide variety of domains (e.g. Gao et al. 2002).
---------------------------------------------------
P06-1029:5	125:202	For each phonetic string A in adaptation training data, we produced a lattice of candidate word strings W using the baseline system described in (Gao et al. 2002), which uses a word trigram model trained via MLE on the Nikkei Newspaper corpus.
---------------------------------------------------
H05-1034:6	21:160	The four models we compare are a maximum a posteriori (MAP) method and three discriminative training methods, namely the boosting algorithm (Collins, 2000), the average perceptron (Collins, 2002) and the minimum sample risk method (Gao et al. , 2005).
---------------------------------------------------
H05-1034:7	28:160	The task of automatic conversion has been the subject of language modeling research in the context of Pinyin-to-Character conversion in Chinese (Gao et al. , 2002a) and Kana-Kanji conversion in Japanese (Gao et al. , 2002b).
---------------------------------------------------
H05-1034:8	41:160	For a detailed description of each algorithm, readers are referred to Collins (2000) for the boosting algorithm, Collins (2002) for perceptron learning, and Gao et al.
---------------------------------------------------
P03-1066:9	160:223	The EM-like method for learning dependency relations described in Section 3.3 has also been applied to other tasks such as hidden Markov model training (Rabiner, 1989), syntactic relation learning (Yuret, 1998), and Chinese word segmentation (Gao et al. , 2002a).
---------------------------------------------------
