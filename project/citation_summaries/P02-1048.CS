1 Introduction Multimodal conversational systems promote more natural and effective human machine communication by allowing users to interact with systems through multiple modalities such as speech and gesture (Cohen et al. , 1996; Johnston et al. , 2002; Pieraccini et al. , 2004). 
These are then combined and assigned a meaning representation using a multimodal language processing architecture based on finite-state techniques (MMFST) (Johnston and Bangalore, 2000; Johnston et al. , 2002b). 
This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al. , 2002b), which re-ranks them in accordance with the current dialogue state. 
Figure 2: Multimodal Architecture 2.2 Multimodal Integration and Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al. , 2002b; Johnston et al. , 2002a) is an extension of the finite-state approach previously proposed (Bangalore and Johnston, 2000; Johnston and Bangalore, 2000). 
1 Introduction and Related Work Dialog agents have been developed for a variety of navigation domains such as in-car driving directions (Dale et al. , 2003), tourist information portals (Johnston et al. , 2002) and pedestrian navigation (Muller, 2002). 
