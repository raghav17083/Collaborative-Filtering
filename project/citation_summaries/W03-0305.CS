5 Experiments Table 2 provides a comparison of our baseline systems using the refined symmetrization metric with the best limited resources track system from WPT03 (Dejean et al. , 2003) on the 2003 test set.


The HLT-03 best is our earlier work (Dejean et al. , 2003), simply based on IBM4 alignment using an additional lexicon extracted from the corpus.


On limited resources, Ralign.EF.1 (Simard and Langlais, 2003) produced the best F-score, as well as the best AER when NULL alignments were taken into account, while XRCE.Nolem.EF.3 (Dejean et al. , 2003) produced the best AER when NULL alignments were discounted.


It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al. , 2003; Mihalcea and Pedersen, 2003).


Team System name Description Language Technologies Institute, CMU BiBr (Zhao and Vogel, 2003) MITRE Corporation Fourday (Henderson, 2003) RALI Universite the Montreal Ralign (Simard and Langlais, 2003) Romanian Academy Institute of Artificial Intelligence RACAI (Tufis et al. , 2003) University of Alberta ProAlign (Lin and Cherry, 2003) University of Minnesota, Duluth UMD (Thomson McInnes and Pedersen, 2003) Xerox Research Centre Europe XRCE (Dejean et al. , 2003) Table 1: Teams participating in the word alignment shared task We conducted therefore 14 evaluations for each submission file: AER, Sure/Probable Precision, Sure/Probable Recall, and Sure/Probable F-measure, with a different figure determined for NULL-Align and NO-NULL-Align alignments.


