While this work represents an important step in reducing the cost of constructing an L2P training set, we intend to explore other active learners and classification algorithms, including sequence labelling strategies (Settles and Craven, 2008). 
AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. 
There certainly are many other modeling strategies that could be used, for example a conditional random field (as in Settles and Craven (2008)), or a model that deals differently with POS labels and morpheme gloss labels. 
Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). 
It is important to note that empirical comparisons of different active learning techniques have shown that random sampling establishes a very 132 0 5 10 15 20Number of training words (x100) 10 20 30 40 50 60 70 80 90 100 Word accuracy (%)  Context OrderingNo Context Ordering (a) Context Ordering 0 5 10 15 20Number of training words (x100) 10 20 30 40 50 60 70 80 90 100 Word accuracy (%)  ClusteringNo Clustering (b) Clustering 0 5 10 15 20Number of training words (x100) 10 20 30 40 50 60 70 80 90 100 Word accuracy (%)  Query-by-BaggingRandom Sampling (c) Active learning 0 5 10 15 20Number of training words (x100) 10 20 30 40 50 60 70 80 90 100 Word accuracy (%)  ALINEEM (d) L2P alignment Spanish Italian+French Dutch+GermanEnglish Figure 1: Performance of the individual system components strong baseline on some datasets (Schein and Ungar, 2007; Settles and Craven, 2008). 
