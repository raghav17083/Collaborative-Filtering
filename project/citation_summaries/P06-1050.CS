To extract such implicit event duration information from texts automatically, we developed a corpus annotated with typical durations of events (Pan et al. , 2006a) which currently contains all the 48 non-Wall-Street-Journal (non-WSJ) news articles (a total of 2132 event instances), as well as 10 WSJ articles (156 event instances), from the TimeBank corpus annotated in TimeML (Pustejovky et al. , 2003). 
These are described in detail in (Pan et al. , 2006a). 
Because the annotated corpus is still fairly small, we cannot hope to learn to make finegrained judgments of event durations that are currently annotated in the corpus, but as we show in greater detail in (Pan et al. , 2006b), it is possible to learn useful coarse-grained judgments that considerably outperform a baseline and approach human performance. 
Experimental results show that the use of the annotation guidelines resulted in about 10% improvement in inter-annotator agreement, measured as described in this section, see (Pan et al. , 2006a) for details. 
In ongoing work, Jerry Hobbs and his colleagues (Pan et al. 2006) have developed an annotation scheme for humans to mark up event durations in documents. 
