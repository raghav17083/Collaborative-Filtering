In the English all-words task of the previous SENSEVAL evaluations (SENSEVAL-2, SENSEVAL3, SemEval-2007), the best performing English all-words task systems with the highest WSD accuracy were trained on SEMCOR (Mihalcea and Moldovan, 2001; Decadt et al., 2004; Chan et al., 2007b). 
We considered the three best-ranking WSD systems  GAMBL (Decadt et al. , 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc Table 4: Performance of WSD systems at Senseval-3 on coarse-grained sense inventories. 
Although more general than models that are built individually for each word in a test corpus (Decadt et al. , 2004), the applicability of the semantic models built as part of SENSELEARNER is still limited to those words previously seen in the training corpus, and therefore their overall coverage is not 100%. 
In SENSEVAL-2, the best performing system (Mihalcea and Moldovan, 2001) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system (Decadt et al., 2004) achieved an accuracy of 65.2%. 
NO-EDGE model uses only the vertex features, while each of the Sn-EDGE models makes use of the edge features associated with System Recall PNNL (Tratz et al., 2007) 67.0% Simil-Prime (Kohomban and Lee, 2005) 66.1% ALL-EDGE 65.5% GAMBL (Decadt et al., 2004) 65.2% SENSELEARNER (Mihalcea et al.,2004) 64.6% BASELINE 62.2% Table 3: The comparison of the performance of WSD systems evaluated on the SENSEVAL-3 English all-words test set. 
