In all other respects, our work departs from previous research on broad--coverage 16 I t I I I I I i ! I i I I I I I I I I I I I i I 1, I. I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al. , 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al. , 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al. , 1993a; GrinBerg et al. , 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. 
2.1 Event Representation of Parse Trees A statistical parser computesa2a4a3a6a5a8a7a9a11a10, the probability of a parse a5 given a sentence a9 . Since the space of the entire parses is too large and cannot be modeled directly, a parse tree a5 is decomposed as a series of individual actions a12a14a13a16a15a17a12a19a18a20a15a22a21a23a21a22a21a24a15a17a12a19a25a27a26 . In the parser (Jelinek et al. , 1994) we used in this study, this is accomplished through a bottomup-left-most (BULM) derivation. 
(1994), Magerman (1995), and Ratnaparkhi (1997) We now make a detailed comparison of our models to the history-based models of Ratnaparkhi (1997), Jelinek et al. 
The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994). 
for IBM Manua!s task; see Table 3 above I i I I I I I i I I I parsing results by (Jelinek et al. , 1994). 
