The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al. , 2002; Xu et al. , 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words.


Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al. , 2002; Xu et al. , 2003).


They have previously been shown to outperform standard back-off models in terms of perplexity and word error rate on medium and large speech recognition tasks (Xu et al. , 2003; Emami and Jelinek, 2004; Schwenk and Gauvain, 2004; Schwenk, 2005).


