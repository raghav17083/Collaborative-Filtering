As an additional baseline we use a bilingual model (Snyder et al., 2008). 
There1Another difference is that we use the English lexicon provided with the Multext-East corpus, whereas (Snyder et al., 2008) augment this lexicon with tags found in WSJ. 
Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al., 2008), and the multilingual model presented here. 
One possible explanation for this decrease lies in the fact that English, by far, has the lowest trigram tag entropy of all eight languages (Snyder et al., 2008). 
Previous work has leveraged this idea by building models for unsupervised learning from aligned bilingual data (Snyder et al., 2008). 
