An interesting example is the recent system by Smith and Eisner (2008), which used loopy belief propagation in a dependency parser using highly complex features, while still maintaining cubic-time search complexity.


An appealing alternative to a similar approximation to the above would use loopy belief propagation to efficiently approximate the marginals (Smith and Eisner, 2008).


Some stem from work on graphical models,includingloopybeliefpropagation(Suttonand McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006).


Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t,a,tprime | w,wprime) that evaluates the full graph of dependency and alignment edges.


SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008).


