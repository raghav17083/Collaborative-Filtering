In which case, the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005; Wang et al. , 2006).


In the second approach, I improve structured large margin training for parsing in two ways (Wang et al. , 2006).


1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007).


3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006).


