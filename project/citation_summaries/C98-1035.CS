We are concerned here with probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of Chelba and Jelinek (1998). 
As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. 
Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al. , 1999). 
Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). 
Following previous work on structured language modeling (Roark,2001;Charniak,2001;ChelbaandJelinek, 1998), we therefore trained the parser on sections 221 of the Penn Treebank containing 936,017 words. 
