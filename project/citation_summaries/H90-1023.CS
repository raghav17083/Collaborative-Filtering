The objective metrics that have been used to evaluate a dialog as a whole include (Abella, Brown, and Buntschuh, 1996; Ciaremella, 1993; Danieli and Gerbino, 1995; Hirschman et al. , 1990; Hirschman et al. , 1993; Polifroni et al. , 1992; Price et al. , 1992; Smith and Hipp, 1994; Smith and Gordon, 1997; Walker, 1996):  percentage of correct answers with respect to a set of reference answers  transaction success, task completion, or quality of solution  number of turns or utterances;  dialogue time or task completion time  mean user response time  mean system response time  frequency of diagnostic error messages  percentage of "non-trivial" (more than one word) utterances. 
Recent work includes discussion of appropriate statistical methods and metrics for spoken dialogue systems (Bates and Ayuso 1993; Danieli et al. 1992; Hirschman et al. 1990; Hirschman and Pao 1993; Simpson and Fraser 1993), information extraction systems (Lewis 1991; Chinchor, Hirschman, and Lewis 1993; Chinchor and Sundheim 1995), and tagging reliability (Carletta 1996). 
One widely used approach to evaluation is based on the notion of a reference answer (Hirschman et al. , 1990). 
While evaluation metrics for these components are well understand (Sparck-Jones and Galliers, 1996; Walker, 1989; Hirschman et al. , 1990), it has been difficult to develop standard metrics for complete systems that integrate all these technologies. 
A wide range of cost measures have been used in previous work; these include pure efficiency measures such as the number of turns or elapsed time to complete the task (Abella, Brown, and Buntschuh, 1996; Hirschman et al. , 1990; Smith and Gordon, 1997; Walker, 1996), as well as measures of qualitative phenomena such as inappropriate or repair utterances (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Simpson and Fraser, 1993). 
