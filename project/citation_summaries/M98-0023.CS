The agreement exhibited by two human judges was measured by analogy with theevaluationemployedfortheMUC-7sharedtasks (Chinchor, 1998).


In this study the agreement exhibited by an annotator a is evaluated as a pair-wise comparison against the other annotator b. Annotator b provides 96 COR Number correct INC Number incorrect MIS Number missing SPU Number spurious POS Number possible = COR + INC + MIS ACT Number actual = COR + INC + SPU FSC F-score = (2  REC  PRE) /(REC + PRE) REC Precision = COR/POS PRE Recall = COR/ACT SUB Substitution = INC/(COR + INC) ERR Error per response = (INC + SPU + MIS) /(COR + INC + SPU + MIS) UND Under-generation = MIS/POS OVG Over-generation = SPU/ACT Table 3: MUC-7 score definitions (Chinchor 1998).


The MUC-7 tasks included extraction of named entities, equivalence classes, attributes, facts and events (Chinchor, 1998).


