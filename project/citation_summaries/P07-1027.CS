F1 Testing F1 Jiang and Ng (2006) 0.6677 0.6914 Liu and Ng (2007) 0.7283 This paper 0.7454 0.7630 Table 1: Markable-only NomBank SRL results for argument prediction using automatically generated parse trees. 
In recent years, NomBank (Meyers et al., 2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machinelearned approaches such as (Pradhan et al., 2004) and (Liu and Ng, 2007), which perform fairly well in the overall task of classifying deverbal arguments. 
Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. 
In consequence, agent/patient nominals tend to realise fewer arguments  the average in FrameNet is 1.46 arguments, compared to 1.74 PropBank Verbs (Carreras and M`arquez, 2005) 80% Nouns (Liu and Ng, 2007) 73% FrameNet Verbs (Mihalcea and Edmonds, 2005) 72% Nouns (Pradhan et al., 2004) 64% Table 1: F-Scores for supervised SRL (end-to-end) for events/results. 
Following the evaluation methodology used by Jiang and Ng (2006) and Liu and Ng (2007), we obtained sig1The syntactic parse can be based on ground-truth annotation or derived automatically, depending on the evaluation. 
