3 Recognizing Textual Entailment Recent work in computational semantics (Haghighi et al. , 2005; Hickl et al. , 2006b; MacCartney et al. , 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual entailment (TE). 
A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al. , 2005; de Salvo Braz et al. , 2005). 
Dependency structures are more ef cient to parse (Eisner, 1996) and are believed to be easier to learn, yet they still capture much of the predicate-argument information needed in applications (Haghighi et al. , 2005), which is one reason for the recent interest in learning these structures (Eisner, 1996; McDonald et al. , 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). 
Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks. 
2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al. , 2005), graph matching (Haghighi et al. , 2005), and label propagation (Niu et al. , 2005). 
