One of the approaches for corpus error detection is use of machine learning techniques (Abney et al. , 1999; Matsumoto and Yamashita, 2000; Ma et al. , 2001). 
Some approaches rely on k-order generative probabilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). 
Although boosting has not yet been applied to coreference resolution, it has outperformed stateof-the-art systems for NLP tasks such as partofspeech tagging and prepositional phrase attachment (Abney et al. , 1999), word sense disambiguation (Escudero et al. , 2000), and named entity recognition (Carreras et al. , 2002). 
PP-Attachment Accuracies of Previous Work method accuracy our method SVM 87.25% supervised Ratnaphakhi et al., 1994 ME 81.6% Brill and Resnik, 1994 TBL 81.9% Collins and Brooks, 1995 back-o 84.5% Zavrel et al., 1997 NN 84.4% Stetina and Nagao, 1997 DT 88.1% Abney et al., 1999 boosting 84.6% Vanschoenwinkel and Manderick, 2003 SVM 84.8% Zhao and Lin, 2004 NN 86.5% unsupervised Ratnaparkhi, 1998 81.9% Pantel and Lin, 2000 84.3% ME: Maximum Entropy, TBL: Transformation-Based Learning, DT: Decision Tree, NN: Nearest Neighbor configurations (McNemars test; p<0.05). 
Figure 3 gives pseudocode for the concrete case of the network in figure 1(d); the general case is similar, and is in fact just a max-plus version of standard inference algorithms for Bayes nets (Cowell et al. , 1999, 97). 
