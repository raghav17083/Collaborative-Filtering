This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. 
This leads to the following probabilistic model: P(f | e;a) = summationdisplay I1(a) P(I1) productdisplay fj,ejI1(f,e) P(fj | ej) (1) Where (a) is the set of binarizable segmentations (defined next) that are eligible according to the word-alignments a between f and e. These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al., 2006)) which must generate the alignment on top of the segmentations. 
Each aligned training sentence (x,a,y) thusgeneratesanumberofpotentiallyoverlapping in (DeNero et al., 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. 
The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). 
Using this method for extracting a set of phrase pairs, (DeNero et al., 2006; Moore and Quirk, 2007) aim at defining a better estimator for the probabilities. 
