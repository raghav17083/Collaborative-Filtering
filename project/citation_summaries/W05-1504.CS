The lower section of Table 1 reports the results achieved by our best model on the test data set and compare them both to those obtained by the only unlexicalised dependency model we know of (Eisner and Smith, 2005) and to those achieved by the stateof-the-art dependency parser in (McDonald, 2006). 
We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 117% (absolute) over CE (and 830% over EM), achieving to our knowledge the best results on this task to date. 
By testing models trained with a fixed value of  (for values in [1,1]), we ascertained that the performance improvement is due largely to annealing, not just the injection of segmentation bias (fourth vs. fifth column of Table 3).8 6 Comparison and Combination with Contrastive Estimation Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by EM. 
All features are all of the form given earlier, except for length features (Eisner & Smith, 2005). 
Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. 
