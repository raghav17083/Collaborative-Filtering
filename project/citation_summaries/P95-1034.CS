(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). 
1 Introduction To date, only limited use of statistically-derived resources has been made for realization in natural language generation, notably Knight & Hatzivassiloglou (1995), Langkilde & Knight (1998) and Bangalore & Rambow (2000). 
Over the years, several proposals of generic NLG systems have been made: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al. , 2002), etc. Instead of relying on such generic NLG systems, however, most of the current text-to-text applications use other means to address the generation need. 
This approach, originally proposed by Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used in statistical generation. 
The generation task is to find a sentence e such that (1) e is a good sentence a priori, and (2) its meaning is the same as the input MR. For thelanguagemodel,weuseann-grammodel,which is remarkably useful in ranking candidate generated sentences (Knight and Hatzivassiloglou, 1995; Bangalore et al. , 2000; Langkilde-Geary, 2002). 
