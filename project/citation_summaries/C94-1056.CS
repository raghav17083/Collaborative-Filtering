COSY-MATS: An Intelligent and Scalable Summarisation Shell Maria Aretoulaki Dept of Pattern Recognlt~on (Computer ~Sc~ence 5) Unlvers~ty of Erlangen-Nuremberg Martensstrasse 3 D 91058 Erlangen, Germany aret oula@~nformat ~k. un~-erlangen de Abstract In tins paper, an architecture is presented for robust and portable summansatlon, COSY-MATS COSY-MATS Can avmd the superfimahty and domain-dependence of IE approaches by means of lngh-level (pragmatic and rhetorical) content selectmn features It can also obviate the text typedependence and cumbersome computation revolved m NLU-based snmmansatl0n systems, because surface criteria are add~t~onally used m the content selectmn process, as are ~dent~fied mappings between those and the htgh-level features In ths way, COSY-MATS should retain ~ts generic and scalable character, wlnle also pernuttmg mtelhgent apphcatmn-spec~Sc processing 1 Motivations behind the Design of COSY-MATS The goal of the research reported here has been to develop a fleable, easdy-portable and scalable, but also efficient and robust, NLP system that automatically generates summaries of real-world unrestricted texts To tins effect, an archttecture was designed for a hybrid COnnectmmst-SYmbohc MAchine for Text Summansatlon (henceforth, COSY-1VIATS) (Axetoulah, 1996) A major concern m des~gmng COSY-MATS has been to identify content selectmn features that are generic and apphcatmn-mdependent (Section 2) The features should be apphcable to any text, Irrespective of domain or text type Tins is so that COSY-MATS 1s readdy portable to dflferent operation ehv~onments vath a nnmmum amount of cnstonnsa~ tton The isolation of such features would provzde a permanent infrastructure for both content selection and analysis The front-end text analysis modules can be developed so that they are geared towards the s,mmansatlon task, rather than text understandmg m general, wlnch is computatlonally-mteuslve Thus, these modules need only perform an analysis that 1s suiBc~ent for the evaluation of the selected content selectlon features The estabhshment of umversal unportance determlnatlon criteria means that the permanent set of analysis, interpretation, content selection and generation processors can be extended with apphcatlon-specfflc modules dunng the porting of COSY-MATS Tlns m also what renders COSY-MATS a type of summartsatson shell Slgmficantly, the computatlons of the supplementary modules wdl already be accommodated for m the standard flow of processing of the system by wrtue of these features (Section 3) Admittedly, the ldentxficatlon of content selectlon features of general apphcablhty is a very d,mcnlt task Tins is demonstrated m the lnmtatlons of the two mare trends m current surnmansatlon research (cf (Aretoulah, 1996)) There are In/ormatzon Eztrachon (Is) enwronments, wlnch perform a superiiclal and partial analysis of the input text based on the progressmn of keywords and apphcatmn-specfllc phrasal patterns thereto, e g (BT, 1994, Jacobs and Rau, 1990, L,lhn, 1958, MUC-5, 1993, Palce, 1981, Patce, 1990, Salton et al, 1994) The problem with IE systems ~s that, although they can be used very efllclently on any type of text, they are domain-dependent and hkely to produce maccurate output Ths ~s due to their excessive rehance on speclahsed content words There are also systems winch are based on Natural Language Understandsng (NLU) methods revolving deeper processmg Apart from syntactic and lexlco-semantlc analysis, the lnerarclncal rhetorical orgamsatlon of the source text can also be taken into account, as can certain aspects of the context of the dmcourse, e g (Ganghano et al, 1993, Lehnert, 1981, lVIltkov et al, 1994) Such more soplnstlcated types of system, however, are prohlbltlvely slow as a result of the extenmve processing revolved They are also very fragde, because the hlgh-level knowledge em ployed is usually hand-coded and hence arbitrary and incomplete Even when this knowledge has been acqurred automatically, e g (Maybury, 1993, Soderland and Lehnert, 1994), it is apphcatlondependent Consequently, despite their occasional 74 I i i I l I I ! I domain-independence (e g (En&es-Nlggemeyer and Neugebauer, 1995, Ono et al, 1994, Rau et al, 1993, Sharp, 1991)), NLU approaches are ---on the whole-specmhsed m a particular text-type For the demgn of COSY-MATS, a ~o~$c and umfymg approach has been adopted that revolves both extrahngmst:c, NLU-type, analysts and selective statmt~cs-based lmgmst:c processing reminiscent of IZ, m co-ordination S~mdarly to NLU, analysts m COSY-MATS is sufl~cmntly deep for the semantic, rhetorical and contextual aspects of the input text to be considered m content selection In contrast to what the case ts with such systems, however, the computation of these d~verse aspects of the text ts efficmnt Thin ts because objective cues on the surface of the text are also explmted m COSY-MATS, echoing. 
And, yet, others assume that important sentences and clauses are derivable from a discourse representation of texts (Ono et al. , 1994; Marcu, 1997a; Marcu, 1997c). 
the I~ approach Nevertheless, unhke rE, these cues are function words'and g~nenc content words winch point towards the ingh-level functmus of the respective textual umts m the context of the dmcourse, while at the same tnne being domain-independent Thus, apart from ldent~fymg umversal content selection criteria that should render COSY-MATS portable and scalable, the research reported here has also attempted to establtsh mappings between the concrete and the more abstract criteria m the devmed feature scheme, so that the system ts also mtelhgent and pruct~cnl, ~ e so that the evaluation of these abstract criteria ts fully automated (Section 2) 2 Intelligent Content Selection Criteria In order to identlfy generic content eelectlon features that can be used by COSY-MATS m any apphcatlon context, an extensive corpus analyms was camed out on a variety of real-world texts Three mare types of text have been analysed newspaper articles, sczent~fic papers and (s~entsfic) author abstracts The subcorpus of newspaper artxcles (160) m extremely dwerse m both ~ts content  and form The topics range from business news and legal reports to social commentary, me&cal msues and pohtlcs Slmalarly, the other two subcorpora constst of 170 articles and abstracts, respectxvely, that pertain to scmnttfic fields such as computer science, the natural scmnces, as well as pinlosophy and hngmstxcs In addltmn, the texts are of varying length from half a page m the case of the abstracts and most news agency reports, to four or more pages, when smentlfic papers and newspaper special reports are revolved Consequently, apart from covering a range of subject domama, the corpus used m designing the content selection processes m COSY-MATS a\]8o represents more than two text types The corpus was analysed both on the surface and on more abstract levels Given the chvermty of the types of text and the writing styles exh~inted m the corpus, regulantms regarding the rhetorical develop75 ment of the texts and the central mformat4onal umts thereto could not be easdy estabhshed 0nly m the case of the smenttfic papers and thetr abstracts could any statements be made on the logical progresmon of the presentation of the content, from the purpose of the research, to the methodology, the experimental set-up and the evaluation of any results (d (Gopink, 1972, Jordan, 1993, Lucas et ai, 1993, Mmzell et al, 1971)) The newspaper articles were mainly studmd m terms of groups of ad3acent sentences and the rhetoncal relatlonsinp between them (d (Ono et al, 1994)) No generahsatlons could be made regarding their top-level orgamsatlon A number of theories of pragmatlcs, dmcourse analyms and text development have prowded useful concepts for tins study of the corpus at a Ingher level  a) theories winch are preoccupmd with the comrnun:catsng agents, their goals, plans and behefs, such as Speech Act Theory (Austin, 1962, Searle, 1969), Rhetorical Structure Theory (RAT) (Mann and Thompson, 1987), or AI research on scripts (Lehnert, 1981, Schank and Abelson, 1977) and behef ascnptmn (Wdks and Balhm, 1987)   b) theories on the tracking of the dsscourse h~tory by means of identlfymg the focused items thereto, e g (Grosz, 1986, Hobbs, 1978, Relchman, 1985, S1dner, 1983, Webber, 1983)  c) theories of cohesson and coherence and how these are m~mfested on the surface of the text, e g SysteInlc-~uuctlonal Lmgutstlcs (Halhday and Hasan, 1976) and the Problem-Solutmn mformation metastructure (Hoey, 1994, Jordan, 1984) (cf (Pmce, 1981)) The &verslty of the subject matter covered m the corpus has meant that specmhsed keywords were ignored m its analysm Instead the emphasts was placed on functlon words and.regular generallanguage content words winch are assooated wlth the mstantlatmn of the semantlc, rhetorical and pragmatlc functlous cous~dered Such lemcal xtems can be employed as markers, not only of the development of the dmcourse but also of the focused and central points thereto In thin process, the var1ous cohesion and coherence theorettlcal frameworks were very mfluentlal, as were the computatlonal approaches to focus pre&ctmn and identtficatmn As a result of thin corpus analysm at the sur-. 
Results of discourse analysis can be used to solve many important NLP problems such as anaphoric reference (Hirst 1981), tense and aspect analysis (Hwang and Schubert 1992), intention recognition (Grosz and Sidner 1986; Litman and Allen 1990), or'can be directly applied to computational NLP applications such as text abstraction (Ono et al. 1994; T'sou et al. 1996) and text generation (McKeown 1985; Lin et al. 1991). 
~1 Michael Elhadad Mathemat~s and Computer Saence Dept Ben Gunon Umveraty m the Negev Beer-Sheva, 84105 Israel http //mr cs.bgu ac.xl/ elhadad Abstract We investigate one techmque to produce a summary of an original text without requmng zts full semanttc interpretation, but instead relying on a model of the topic progresston m the text derived from lexlcal chains We present a new algonthm to compute lexlcal chains m a text, merging several robust knowledge sources the WordNet thesaurus, a partof-speech tagger and shallow parser for the identification of nominal groups, and a segmentatton algorithm dernved from (Hearst, 1994) Summarization proceeds m three steps the ongmal text is first segmented, lexxcal chmns are constructed, strong chains are ldsnhfied and ssgnzflcant sentences are extracted from the text We present m tins paper empirical results on the tdent~catlon of strong chains and of slgmfieant sentences Introduction Summarization ts the process of condensing a source text into a shorter Version preserving its reformation content It can serve several goals -from survey analysis of a sctenttfic field to qmck mchcatzve notes on the general toplc of a text Producing a quahty reformative summary of an arbitrary text remams a challenge winch reqmres full understanding of the text Indtcattves, lm~artes, winch can be used to qmckly decide whether a text is worth reading, are naturally easter to produce In tins paper we investigate a method for the production of such mdxcatlve summaries from arintrary text (Jones, 1993) descnbes summarization as a twostep process (1) Building from the source text a source representatton, (2) Summary generationfonmng summary representation from the source representation bmlt m the first step and synthesismg the output summary text Within this framework, the relevantquestion is what reformation has to be included m the source representation m order to create a summary There are three types of source text reformation hngmstlc, domain and commumcatlve Each of these text aspects can be chosen as a barns for source representatlon Summaries can be bmlt on a deep semantic anal= ysis of the source text For example, (McKcown and Radsv, !905)investigate ways to produce a coherent summary of several texts describing the same event, when a detaded semantic representation of the source texts m available (m their case, they use MUC-style systems to interpret the source texts) Alternatzvely, early summarisatzon systems (Luhn, 1968) used only hngumtlc source mformation The mtmtlon was that the moat frequent words represent the tmportant concepts of the text In this approach the source representation was the frequency table of text words Tins representation abstracts the text into the umon of its words w~thout conmdermg any connectlon among them In contrast to these two extreme pcsltlous (using as a source representation a full semantic representation of the text or reducing ltto a simple frequency table), we deal m tins paper wttb the issue of producmg a summary from an arbitrary text without reqmrmg zts full understanding, but using wtdely avadable knowledge sources Our mare goal is therefore to find a middle ground for source representation, rich enough to braid quality indicative summaries, but easy enough to extract from the source text to work on arbltrary text Over-slmphficatlon can harm the quahty of the source representation As a trivial illustration, consider the following two sequences  1 "Dr Kenny has sn~ented an anesthetsc maehsne Thss devwe controls the rate at wh:ch an anaesthctsc ss pumped into the blood" 2 "Dr Kenny has :nvented an anesthet:c machsne The Doctor spent two years on thu research" ~Dr Kenny ~ appears once m both sequences and I0 I I I I i I II II I ! so does ~nach:n ~ But sequence 1 ts about the roach:he, and sequence 2 m about the *doctor ~ Tlus example mchcates that zf the source representation does not supply mformatlon about semantically related terms, one cannot capture the %boutnesg' of the text, and therefore the summary will not capture the mare point of the original text The norton of cohemon, introduced m (Halhday and Hasan, 1976) captures part of the mtmtmn Cohereon is a dewce for "sticking together" different parts of the text Cohesion m achmved through the use of semantmaUy related terms, reference, elhpsm and conjunctlous Among these dtfferent means, the most easdy zdentfllable and the most frequent type m lemcal cohe" slon (as discussed m (Hoey, ~ 1991)) Lexlcal cohesion is created by usmg semantically related words Halhday and Hasan classflled lemcal cohesion into relteratlon category and collocatlon category Rezteratlon can be achmved by repetltlon, synonyms and hyponyms Collocatmn relatzons spectfy the relation between words that tend to co-occur m the same lexzeal contexts (e g, "She works as a teacher m the.School ~) Collocation relations are more problematzc for ldenttticat~on than rezterat|on, but both of t~hese categones are Identifiable on the surface oi~ the text Lextcal cohemon occurs not only between two terms, but among sequences of related words ~ called/ez~cal chains (Morns and Hlrst, 1991) Lemcal chains provide a representahon of the lemcal cohemve structare of the text Lemcal chains have also been used for mfo~nahon retrieval (Stamnand, 1996) and for correction ofmalaproptsms (Htrst and St-Onge, 1997 (to appear)) In tlus paper, we mveshgate how lemcal chmns can be used as a source representation for summarization Another nnportant dunenmon of the lmgumtzc structure of a source,text m captured under the related not,on of coherence Coherence defines the macro-level semantic structure of a connected dLscourse, while cohesion creates connectedness m a non-structural manner Coherence m represented m terms of coherence relat~ous between text segments, such as cla~orahon, cause and ezplanat|on Some researchers, e g, (Ono, Kazuo, and Seljl, 1994), use chscourse structure (encoded umng RST (MAnn and Thompson, 1987) as a source representatxon for summanzatxon) Clearly, thin representation ms expresmve enough, the question m whether ~t m computable In contrast to lemcal cohemon, coherence m chfl~cult to zdent|fy mthout complete understandmg of the text and complex reference In ad&tton, there m no prease criteria for clasmficat~on of different relatlous Consider the following example from Hobbs(1978) "John can open the safe He Imows the combmahon " (Morns and H~mt, 1991) show that the relation between these two sentences can he interpreted as daborahon or as ezplanahon, depen&ng on %ontext, knowledge and behefs" There m, however, a close connechon between dincourse structure and cohemon Related words tend to co-occur mthm a dmcourse umt of the text So cohemon m one of the surface mgns of dmcourse structure and lexlcal chaln~ can' be used to Identify it Other mgns can be used to ldentzfy dmcourse structure as well (connect,yes, paragraph markers, tense shifts) In thls paper, we investigate the use oflemcal chains as a model of the source text for the purpose of producing a summary Obviously, other pects of the source text need to be integrated m the text representation to produce quahty summaries, but we want to empmcally investigate how far one can go exploiting mainly lemcal chains In the rest of the paper we first present our algorithm for lexzeal chain construct,on We then present empmcal results on the ldentlficatzon of strong chains among the posmble can&dates produced by our algorithm Finally, we describe how lexlcal chains are used to identify mgmficant sentences mtlnn the source text and eventually produce a surQmary Algorithm for Chain Computing One of the clnef advantages of lemcal cohesmn m that zt m an easdy reco~m~able relatmn, enabhng lexlcal chains computation The first computational model for lemcal chains was presented m (Morns and Hlrst, 1991) They define lexlcal cohesmn relatzons m terms of categories, index entries and pointers m Roget's Thesaurus Morns and Hlrst evaluated that their relatedness criterion covered over 90% of the mtmttve lexzcal relatzons Cham~ are created by taking a new text word and findtng a related chain for it according to relatedness criteria Morns and HLrst introduce the notion of "actzvated chain ~ and ~cham returns", to take into account the dmtance between occurrences of related words They also analyze factors contributing to the strength of a chain -repetltxon, density and length Morns and Hn'st &d not ~nplement their algorithm, because there was no machine-readable vermon of Roget's Thesaurus at the tzme One of the drawbacks of thelr approach was that they chd not reqmre the same word to appear ruth the same sense m ~ts &ffexent occurrences for tt to belong to a chain For semantically ambiguous 11 words, this can lead to confnslous (e g, mixing two senses of taSle as aptece 0f furniture or an array) Note that choosing the appropriate chain for a word is eqmvalent to dzsamblguatmg tins word m context, which is a well-known d~fl~cult problem m text understanding More recently, two algorithms for the calculation of lexlcal chains have been presented m Hirst and StOnge (1995) and Stairmand (1996) Both of these algornthms use the WordNet lexlcal database for determining relatedness of the words (Miller et al, 1990) Senses m the WordNet database are represented relatlonally by synonym sets ('synsets') -which are the sets of all the words sharing a common sense For example two senses of "computer" are represented as {calculator, reckoner, figurer, estimator, computer) (s e, a person who computes) and {computer, data processor, electromc computer, reformation processing system) WordNet contains more than 118,000 dflferent word forms Words of the same category are hnked through semantic relations hke synonymy and hyponymy Polysemous words appear m more than one synsets (for example, comptdcr occurs m two synsets) Approxtmately 17% of the words m WordNet are polysemous But, as noted by Stairmand, this figure is very tmsleadmg "a slguxficant proportion of WordNet nouns are Latin labels for biological entitles, which by their nature are monosemons and our experience wtth the news-report texts we have processed ts that approxtmately half of the nouns encountered are polysemous" (Stairmand, 1996) Generally, a procedure for constructing lexlcal chains follows three steps (1) Select a set of can&date words, (2) For each candldate word, find an appropriate chain relying on a relatedness cute.on among members of the chains, (3) If It is found, insert the word m the chain and update It accorchngly An example of such a procedure was represented by Hlrst and St-Onge (H&S) In the preprocessor step, all words that appear as a noun entry m WordNet are chosen Relatedness of words xs dstermmed m terms of the distance between their occurrences and the shape of the path connecting them m the WordNet thesaurus Three kinds of relation are defined extra-strong (between a word and tts repetxt~on), strong (between two words connected by a Wordnet relatxon) and mechum-stroug when the hnk between the synsets of the words is longer than one (only paths satisfying certain restrictions are accepted as vahd connectxons) The maxtmum distance between related words depends on the kind of relatxon for extra-strong relattons, there is not hxmt m &stance, for strong relatlons, it is hmlted to a window of seven sentences, and for mechum-strong relations, It is wltinn three sentences back To find a chain m winch to insert a given candtdate word, extra-strong relattons are preferred to strong-relations and both of them are preferred to medmm-strong relations If a chain is found, then the candtdate word is inserted with the appropriate sense, and the senses of the other words m the receiving chain are updated, so that every word connected to the new word m the chain relates to Its selected senses only If no chaan is found, then a new chain Is created and the can&date word ts inserted with all its possible senses m WordNet The greedy &samblguatzon strategy Implemented m this algorithm has some lmntatlonsdinstrated by the following example Mr. Kenny ~s the person that invented an anaesthehc machine whsch uses micro-computers to control the rate at whsch an anaesthehc,s pumped into the blood Such machines are nothing new But hu device uses two micro-computers to achseee much closer momtormg o/the pump \]eedmg the anaesthehc into the pahent Accor&ug to H&S's algorithm, the chain for the word "Mr" is first created \[lex "Kr.", sense {mzster, Mr. }\] "Mr" belongs only to one synset, so it is chsamblguated from the beginning The word "person" is related to tins chain m the sense "a human be,ng" by a medmm-stroug relation, so the chain now contains two entries \[lex "Mr'.", sense {m.ster, Mr.)\] \[lex "person", sense {person, :t.nd~.v~dual, someone, man, mortal, huma.u, sou1}\] When the algorithm processes the word "machineD, It relates it to this cham, because "roach:hen m the first WordNet sense ("an e Oiczent person") is a holonym of apersonn m the chosen sense In other words, "machine" and "person" are related by a strong relation In tins case, "machine" ts disamblguated m the wrong way, even though after tins first occurrence of "machine", there is strong evidence supporting the selechon of xts more common sense "macro-computer", "demce" and "pemp" all point to its correct sense m tins context ~ "any mechanzcal or electrzcal devzce thaZ performs or assgs~s zn the performance" Tins example mdtcates that disamblguatlon cannot be a greedy decision In order to choose the right sense of the word the 'whole ptcture' of chain distnbutwn m the text must be conmdered We propose to develop a chaining model according to all possxble alternatives of word senses and then choose the best one among them Let us dlustrate tins method on the above exam12 I I I I II II II I I II I ! II ! pie First, a node for the word =Mr" Is created \[lex "ltr'.", sense {mister, Kr }3 The next candidate word Is "person" It has two senses "haman besng n (person "1) and erratum=heal cafegory of pronouns and verb forms" (person -2) The choice of sense for ~person" sphts the chain world to two dflferent interpretations as shown m Figure 1 I Figure I Step I lpen%} Interpretations 1 and 2 We define a component as a list of interpretations that are exclusive of each other Component Words influence each other in the selection of their respective senses The next candidate word =anaesthetsc" Is not related to any word m the first component, so we cxeate a new component for it with a single lntexpretataon The word "machsne" has 5 senses mach:nei to machine5 In its first sense; "an e.0ic:ent person", it m related to the senses =person" and =Mr" It therefore influences the selection of thexr senses, thus "machine" has to be ~ m the first component After its msertmn the picture of the first component becomes the one shown m Figure 2  But ff we continue the process and insert the words =micro-compeer', = dcmce n and =pump', the number of nlternatlve greatly increases The strongest interpretations are given m Figures 3 and 4 Under the assumption that the text Is cohessve, we define the best interpretation as the interpretation with the most connections (edges m the graph) In tins case, the second interpretation at the end of Step 3 is selected, which predicts the right sense for "machine" We define the score of an interpretation as the sum of its chain scores Chain seore is determined by the number and weight of the relations between chain members Expenlnentally, we fixed the weight of reiteration and synonym to 10, of antonym to 7, and of hyperonym and holonym to 4' Our algorithm develops all possible interpretations, maintainmg each one without self contradiction When the number of possible interpretations is larger than a certain threshold, we prune the weak interpretations according to tins criteria In the end, we select from each component, the strongest interpretation (Mr .m| \[pe~ntlZlt mdtvtdttal mmeoae I I maclune a } S Step 2: Interpretauon 1 tMr,numer} lpenm} {marina% machine s I" Step 2: Interpretation 2 (pez-~ individual me, ) (~ehme a m~h,ne s ) Step 2" InterpretaUon 3 iMr.n~tef} \[permn} {n,aclane, I Step 2: Interpret=non 4 FFtgure 2 Step 2 Interpretations I to 4 .'~.-. 
