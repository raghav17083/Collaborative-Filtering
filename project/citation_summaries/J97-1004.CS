Evaluations of text properties are typically done by asking human judges to rate the quality of generated texts (for example, (Lester and Porter, 1997)); sometimes human-authored texts are included in the rated set (without judges knowing which texts are human-authored) to provide a baseline. 
For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system's performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through "task efficacy" measures (Young 1997). 
(2001), Bontcheva and Wilks (2001), and Lester and Porter (1997)). 
In the Rex approach, the obvious problem is that in order to generate an explanation, additional reasoning must be performed which in some sense is very similar to that done by the expert 2We do not consider explanation generation from data bases (for example, (McKeown, i985; Paris, 1988; Lester and Porter, 1997)) to be the same problem as expert system reasoning explanation (even though we may use some similar techniques). 
2 Background 2.1 Evaluating NLG Systems Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include:  Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999])  Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997])  Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al, 2000]). 
