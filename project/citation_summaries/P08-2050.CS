On the one hand, simply comparing system outputs to a gold standard is not appropriate because there can be multiple generated outputs that are equally good, and finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). 
Even evaluation experiments involving human subjects do not have to come with an exorbitant price-tag: REG08, a competition in the eld of referring expression generation which had very minimal funding, included a task-performance experiment in which the speed and accuracy with which subjects were able to identify intended referents was tested (Gatt, Belz, and Kow 2008). 
In a recent set of evaluation experiments involving 15 NLG systems, the eight intrinsic measures tested (although correlating strongly and positively with each other) either did not correlate signicantly with the three extrinsic measures of task performance that were also tested, or were negatively correlated with them (Belz and Gatt 2008). 
Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). 
In this experiment the intended referent was not highlighted in the onscreen display, and the participants task was to identify the intended referent among the pictures by mouse-clicking on it.10 In previous TUNA identification experiments (Belz and Gatt, 2007; Gatt et al., 2008), subjects had to read the description before identifying the intended referent. 
