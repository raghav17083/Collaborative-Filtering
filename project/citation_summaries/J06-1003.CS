(2006) to 0.85 (word-pair based) in Budanitsky and Hirst (2006). 
1 Introduction The propensity of words to appear together in texts, also known as their distributional similarity is an important part of Natural Language Processing (NLP): The need to determine semantic relatedness between two lexically expressed concepts is a problem that pervades much of [NLP].             (Budanitsky and Hirst 2006)   2008. 
This paper relies on the Jiang-Conrath composite measure (Jiang and Conrath, 1997), which has been shown to be superior to other measures (Budanitsky and Hirst, 2006), and we also found that this measure works the best for the purpose. 
Various lexical resource-based (Budanitsky and Hirst, 2006) and distributional measures (Mohammad and Hirst, 2005) have been proposed to measure semantic relatedness and distance between terms. 
Budanitsky and Hirst (2006) define them as follows: word-pairs are considered to be semantically similar if a synonymy or hypernymy relation holds. 
