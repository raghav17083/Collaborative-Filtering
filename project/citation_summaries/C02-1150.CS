P06-1063:1	44:174	The CCG 5500 questions come from a number of sources (Li and Roth, 2002) and some of these questions contain minor grammatical mistakes so that, in essence, this corpus is more representative of genuine questions that would be put to a working QA system.
---------------------------------------------------
C04-1201:2	128:140	For instance Zhang and Sun Lee (Zhang and Lee, 2003) reported an accuracy of 90% for English questions, while Li and Roth (Li and Roth, 2002) achieved 98.8% accuracy.
---------------------------------------------------
C04-1201:3	33:140	Li and Roth reported a hierarchical approach for question classification based on the SNoW learning architecture (Li and Roth, 2002).
---------------------------------------------------
C04-1201:4	43:140	What is new, however, is that we are using the Internet to obtain values for features in our question classification process, as opposed to previous approaches where the redundancy of information available on the Internet has been used in the answer extraction process (Brill et al. , 2002; Lin et al. , 2002; Katz et al. , 2003).
---------------------------------------------------
C04-1201:5	16:140	Thus it is not surprising that an increasing interest has arisen aimed at developing accurate question classifiers (Zhang and Lee, 2003; Li and Roth, 2002; Suzuki et al. , 2003).
---------------------------------------------------
H05-1040:6	112:213	Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task.
---------------------------------------------------
H05-1040:7	23:213	Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al. , 1999).
---------------------------------------------------
H05-1040:8	16:213	With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1.
---------------------------------------------------
D07-1002:9	147:241	Answer types are determined using classification rules similar to Li and Roth (2002).
---------------------------------------------------
D09-1057:10	82:206	Compared tothe over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be moreinformativeassuggestedbytheexperiments.
---------------------------------------------------
D09-1057:11	17:206	As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008).
---------------------------------------------------
D09-1057:12	40:206	3 Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture.
---------------------------------------------------
D09-1057:13	54:206	This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example).
---------------------------------------------------
E06-1050:14	10:193	Many previous approaches to answer typing, e.g., (Ittycheriah et al. , 2001; Li and Roth, 2002; Krishnan et al. , 2005), employ a predefined set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.
---------------------------------------------------
E06-1050:15	44:193	In a similar experiment, Li & Roth (2002) train a question classifier based on a modified version of SNoW using a richer set of answer types than Ittycheriah et al. The LCC system (Harabagiu et al. , 2003) combines fixed types with a novel loop-back strategy.
---------------------------------------------------
N07-1065:16	33:184	For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question.
---------------------------------------------------
N07-1065:17	22:184	This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers.
---------------------------------------------------
W09-0204:18	137:160	In 3http://search.cpan.org/dist/WordNet-Similarity 30 Accuracy(%) 1000 2000 3000 4000 5500 BOW 77.1 83.3 87.2 87.3 89.2 TK 80.2 86.2 87.4 88.6 91.2 LATK 80.4 86.5 87.5 88.8 91.6  = 1 WUP 81.3 87.3 88.0 89.8 92.5 RES 81.0 87.1 87.9 89.5 92.2 LIN 81.1 87.0 88.0 89.3 92.4 LSA(k = 50) 80.8 86.9 87.8 89.3 91.7 Table 5: Classification accuracy of different kernels on different data sets this paper we use the same dataset as introduced in(Li and Roth, 2002).
---------------------------------------------------
W03-1208:19	46:156	However, question classification requires much more complicated features than text categorization, as shown by (Li and Roth, 2002).
---------------------------------------------------
W03-1208:20	23:156	This framework is useful for question classification because the works of (Li and Roth, 2002; Suzuki et al. , 2002a) showed that richer information, such as structural and semantical information inside a given question, improves the question classification performance over using the information of just simple key terms.
---------------------------------------------------
W03-1208:21	20:156	This work develops a machine learning approach to question classification (Harabagiu et al. , 2000; Hermjakob, 2001; Li and Roth, 2002).
---------------------------------------------------
W03-1208:22	107:156	Additionally, we evaluated the performance using SNoW3 to compare our method to indirectly the SNoW-based question classifier (Li and Roth, 2002).
---------------------------------------------------
W03-1208:23	135:156	5 Discussion First, we could increase the performance by using the information on named entities and semantic information compared to only using the words, which is the same result given in (Li and Roth, 2002).
---------------------------------------------------
H05-1073:24	52:192	Finally, as further discussed in section 6, the hierarchical case of label assignment requires a sequen580 tial model that further defines levels of coarse versus fine-grained classifiers, as done by (Li and Roth, 2002) for the question classification problem.
---------------------------------------------------
H05-1073:25	178:192	Sequential modeling of simple classifiers has been successfully employed to question classification, for example by (Li and Roth, 2002).
---------------------------------------------------
D08-1097:26	4:239	In contrast to Li and Roth (2002)s approach which makes use of very rich feature space, we propose a compact yet effective feature set.
---------------------------------------------------
D08-1097:27	31:239	Li and Roth (2002) have made use of lexical words, part of speech tags, chunks (non-overlapping phrases), head chunks (the first noun chunk in a question) and named entities.
---------------------------------------------------
D08-1097:28	211:239	Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments.
---------------------------------------------------
D08-1097:29	26:239	More recently, Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture (Khardon et al., 927 1999).
---------------------------------------------------
D08-1097:30	73:239	3.2 Head Word Li and Roth (2002;2006) used head chunks as features.
---------------------------------------------------
W08-1809:31	51:203	The set of fine-grained answer types used here differs from the set of answer types such as Li and Roth (2002) used elsewhere in that the set is openended, and new types can be added for an entity at any time.
---------------------------------------------------
P09-2082:32	59:94	We followed Li & Roth (Li and Roth, 2002) to implement the features for the EAT classifier.
---------------------------------------------------
P09-2082:33	15:94	The classification scheme we propose is based on one dynamic 1ISO/IEC 13250:2003, http://www.isotopicmaps.org/sam/ and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002).
---------------------------------------------------
P09-2082:34	31:94	1500 of those questions come from the Li & Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map.
---------------------------------------------------
W06-1906:35	30:143	(Li and Roth, 2002) propose a system based on SNoW.
---------------------------------------------------
W06-1906:36	141:143	It could be also interested to test the combination between a better QC system, the current one by Li and Roths for instance (Li and Roth, 2002), and our machine translation method.
---------------------------------------------------
W06-1906:37	78:143	2LingPipe is a suite of Java tools designed to perform linguistic analysis on natural language data, available in http://www.alias-i.com/lingpipe 3http://trec.nist.gov EACL 2006 Workshop on Multilingual Question Answering MLQA06 41 The same dataset has been used in other investigations, such as in (Li and Roth, 2002).
---------------------------------------------------
W06-1906:38	106:143	(Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy.
---------------------------------------------------
C08-1061:39	10:297	1  The widely used question category criteria is a two-layered taxonomy developed by Li and Roth (2002) from UIUC.
---------------------------------------------------
C08-1061:40	210:297	We use the matrix defined in Li and Roth (2002) to show the performance errors.
---------------------------------------------------
C08-1061:41	139:297	6 Experiments 6.1 Experiment Settings Data Set: We evaluate the proposed approach on the UIUC data set (Li and Roth, 2002).
---------------------------------------------------
C08-1061:42	204:297	Comparison with related work Table 8 shows the accuracies of the LDCRFs based question classification approach with different feature sets, in comparison with the tree method (Nguyen et al. 2007), the WordNet Method (Metzler and Croft, 2005) and the hierarchical method (Li and Roth, 2002).
---------------------------------------------------
C08-1061:43	59:297	Li and Roth (2002) presented a hierarchical classifier based on the Sparse Network of Winnows (Snow) architecture.
---------------------------------------------------
C08-1061:44	150:297	Evaluation metric: Accuracy performance is widely used to evaluate question classification methods [Li and Roth, 2002; Zhang and Lee, 2003, Melter and Croft, 2004; Nguyen et al. 2007].
---------------------------------------------------
C08-1061:45	121:297	They are expected to work as the class-specific relational features which are semiconstructed by (Li and Roth, 2002).
---------------------------------------------------
E09-1076:46	171:206	Li and Roth (2002) also rely on a rigid set of classes and so run the risk of encountering a new question of an unseen type.
---------------------------------------------------
E09-1076:47	168:206	The approach of Li and Roth (2002) is similar in that it uses learning for answer type detection.
---------------------------------------------------
P08-2029:48	45:69	The experimental datasets were created by submitting the 138 TREC 2001 test questions labeled as description in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs.
---------------------------------------------------
W03-0412:49	195:250	Our approach to QC follows that of (Li and Roth, 2002).
---------------------------------------------------
P04-1016:50	157:193	We evaluated the performance by using data provided by (Li and Roth, 2002) for English and (Suzuki et al. , 2003b) for Japanese question classification and followed the experimental setting used in these papers; namely we use four typical question types, LOCATION, NUMEX, ORGANIZATION, and TIME TOP for JQA, and coarse and fine classes for EQC.
---------------------------------------------------
