4.1 Selecting  with Strapping In order to pick the value of the parameter  for each of the sIB and IDT test experiments, we use strapping (Eisner and Karakos, 2005), which, as we mentioned earlier, is a technique for training a meta-classifier that chooses among possible clusterings. 
To the best of our knowledge, this cross-instance learning problem has only been tackled in (Eisner and Karakos, 2005), whose strapping procedure learns a meta-classifier for distinguishing good from bad clusterings. 
(In the original strapping paper, for example, Eisner and Karakos (2005) generated their collection by bootstrapping word-sense classifiers from 200 different seeds.) 
Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figures of merit on successful learning trajectories, or plausibility criteria (Eisner and Karakos, 2005). 
One option is to perform a tuning that directly minimizes the error on the supervised data sets; another option is to use strapping (Eisner and Karakos, 2005), which builds a classifier that learns to distinguish good from bad clusterings, and then selects the  with the best predicted clustering on the test set. 
