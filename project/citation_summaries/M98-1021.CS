MUC-7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al. , 1998; Borthwick, 1999), notably Mikheev's system, which achieved the best performance of 93.39% on the official NE test data. 
Further details on this can be found in (Mikheev et al. , 1998). 
Both of these beliefs are questionable, as the top MUC 7 performance of 93.39% Entity Precision Recall F-Score Fully Correct protein 77.40% 68.48% 72.67% DNA 66.19% 69.62% 67.86% RNA 72.03% 65.89% 68.83% cell line 59.00% 47.12% 52.40% cell type 62.62% 76.97% 69.06% Overall 71.62% 68.56% 70.06% Left Boundary Correct protein 82.89% 73.34% 77.82% DNA 68.47% 72.01% 70.19% RNA 75.42% 68.99% 72.06% cell line 63.80% 50.96% 56.66% cell type 63.93% 78.57% 70.49% Overall 75.72% 72.48% 74.07% Right Boundary Correct protein 84.70% 74.96% 79.53% DNA 74.43% 78.29% 76.31% RNA 78.81% 72.09% 75.30% cell line 70.2% 56.07% 62.34% cell type 71.68% 88.10% 79.05% Overall 79.65% 76.24% 77.91% Table 2: Results on the evaluation data (Mikheev et al. , 1998) in the domain of newswire text used an easier performance metric where incorrect boundaries were given partial credit, while both the biomedical NER shared tasks to date have used an exact match criterion where one is doubly penalized (both as a FP and as a FN) for incorrect boundaries. 
Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity (Borthwick, 1999; Mikheev et al. , 1998). 
Dynamic lexicon are used by a numer of NERC systems such as ones described in (Mikheev et al. 1998), (McDonald 1996) and (Piskorski et al. 2000). 
