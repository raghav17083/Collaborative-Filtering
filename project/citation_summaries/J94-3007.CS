Espcci~flly in lla~l;m:al language and speech processing, decision tree learning has been apt)lied to many probh,.nls including stress acquisition fl'om texts, gralflmme to phonenm conversion and prosodic phrase, modeling (Daelemans et al. , 1994) (van Santen et al. , 1997) (Lee and Oh, 1999).


Case-based learning algorithms have been used in NLP for context-sensitive parsing (Simmons and Yu, 1992), for text categorization (Riloffand Lehnert, 1994); for lexical tagging tasks like part-of-speech tagging and semantic feature tagging (Daelemans et al. , submitted, Cardie, 1994, Cardie, 1993a); for semantic interpretation (e.g. , concept extraction (Cardie, 1994, Cardie, 1993a)); and for a number of low-level language acquisition tasks, including stress acquisition (Daelemans et al. , 1994) and graphemeto-phoneme conversion (Bosch and Daelemans, 1993).


Examples include the use of decision trees for syntactic analysis (Magerman, 1995), coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995), and cue phrase identification (Litman, 1994); the use of inductive logic programming for learning semantic grammars and building prolog parsers 113 (Zelle and Mooney, 1994; Zelle and Mooney, 1993); the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b), and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al. , submitted).


Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al. , 1994), grapheme-phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al. , 1996b).


