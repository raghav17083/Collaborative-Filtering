For evaluation, we are using the ROUGE evaluation toolkit1, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003). 
3.2 Evaluation Metric We evaluated our summarizer on the DUC test sets using the Rouge automatic scoring metric (Lin and Hovy, 2003). 
It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 
For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). 
This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric (Lin and Hovy, 2003) used in the Document Understanding Conferences2 (DUC). 
