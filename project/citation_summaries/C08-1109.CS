For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions(Brockett et al., 2006) and word order (Metcalf and Meurers, 2006).


Human inter-rater agreement is known to be problematic  Figure 1: Screen shot of ESL Assistant  75 on this task: it is likely to be high in the case of certain user error types, such as over-regularized verb inflection (where the system suggests replacing writed with wrote), but other error types are difficult to evaluate, and much may hinge upon who is performing the evaluation: Tetreault and Chodorow (2008b) report that for the annotation of preposition errors using a single rater as a gold standard, there is the potential to overor underestimate precision by as much as 10%. With these caveats in mind, we employed a single annotator to evaluate system performance on native data from the 1-million-word Chinese Learners of English corpus (Gui and Yang, 2001; 2003).


For the harder problem of prepositions, ESL Assistants accuracy is comparable to those reported by Tetreault and Chodorow (2008a) and De Felice and Pulman (2007).


More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow, 2008a).


