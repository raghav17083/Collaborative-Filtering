W04-0833:1	16:99	This con rms Mihalceas observations (Mihalcea, 2002).
---------------------------------------------------
W04-0833:2	64:99	The rst three items are taken from Mihalceas work (Mihalcea, 2002) which are useful features for most of the words.
---------------------------------------------------
W07-2090:3	77:85	mans et al. , 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al. , 2002; Mihalcea, 2002).
---------------------------------------------------
N09-1004:4	16:224	Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003).
---------------------------------------------------
N09-1004:5	25:224	In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996).
---------------------------------------------------
W04-0827:6	20:122	(2002) and Mihalcea (2002).
---------------------------------------------------
W02-0817:7	78:149	More details on this system can be found in (Mihalcea, 2002b).
---------------------------------------------------
W02-0817:8	146:149	We are considering using a AutoASC/GenCor type of approach to generate sense tagged data with a bootstrapping algorithm (Mihalcea, 2002a).
---------------------------------------------------
P05-3014:9	68:80	The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002), 55 Training SENSEVAL-2 SENSEVAL-3 Model size Precision Recall Precision Recall modelNN1 88058 0.6910 0.3257 0.6624 0.3027 modelNNColl 88058 0.7130 0.3360 0.6813 0.3113 modelVB1 48328 0.4629 0.1037 0.5352 0.1931 modelVBColl 48328 0.4685 0.1049 0.5472 0.1975 modelJJ1 35664 0.6525 0.1215 0.6648 0.1162 modelJJ2 35664 0.6503 0.1211 0.6593 0.1153 modelJJColl 35664 0.6792 0.1265 0.6703 0.1172 model*1/2 207714 0.6481 0.6481 0.6184 0.6184 model*Coll 172050 0.6622 0.6622 0.
---------------------------------------------------
P05-3014:10	59:80	For learning, we are using the Timbl memory based learning algorithm (Daelemans et al. , 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al. , 2002), (Mihalcea, 2002).
---------------------------------------------------
D09-1020:11	71:249	Specifically, given an ambiguous target word, we use the following features from (Mihalcea, 2002): CW : the target word itself CP : POS of the target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages.
---------------------------------------------------
W04-0838:12	47:94	For learning, we are using the Timbl memory based learning algorithm (Daelemans et al. , 2001), which was previously found useful for the task of word sense disambiguation (Mihalcea, 2002).
---------------------------------------------------
H05-1114:13	14:189	Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation.
---------------------------------------------------
W04-0849:14	5:92	In the second Senseval competition, these features figured predominantly among the feature sets for the leading systems (Mihalcea, 2002; Yarowsky et al. , 2001; Seo et al. , 2001).
---------------------------------------------------
W07-2047:15	21:85	After an initial set of experiments the algorithm performance was enhanced using a greedy feature selection algorithm similar to one in (Mihalcea, 2002).
---------------------------------------------------
W07-2047:16	49:85	We created the feature set a1a3a2a9a8 by adding this new feature to a1a3a2a10a6 . In the end we applied a greedy feature selection algorithm to features in a1a3a2a10a8 inspired by (Mihalcea, 2002).
---------------------------------------------------
W07-2047:17	61:85	So we created a greedy feature selection algorithm based on the performance of the SVM classifier (Mihalcea, 2002).
---------------------------------------------------
W04-2405:18	71:194	Table 1 lists commonly used features in word sense disambiguation (list drawn from a larger set of features compiled by (Mihalcea, 2002)).
---------------------------------------------------
