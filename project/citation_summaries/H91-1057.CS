To allow a language model to adapt to its recent context, some researchers have used techniques to update trigram statistics in a dynamic fashion by creating a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example (Jelinek et al. , 1991; Kuhn and de Mori, 1990).


One means of injecting long-range awareness into a language model is by retaining a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example (Jelinek et al. , 1991; Kuhn and de Mori, 1990).


Statistical language models have gained considerable acceptance due to the efficiency demonstrated in the fields in which they have been applied (Bahal et al. , 1983; Jelinek et al. , 1991; Ratnapharkhi, 1998; Borthwick, 1999).


For example, dependencies between words exist at separations greater than those allowed for by trigrams (for which long-distance N-grams \[Jelinek et al, 1991\] are a partial remedy), and associating scores with parsing table states may not model all the important correlations between grammar rules.


1 Introduction The task of language modeling is to predict the next word in a sequence of words (Jelinek et al., 1991).


