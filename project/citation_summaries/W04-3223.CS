6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems. 
Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance. 
Whereas the feature selection approach leads to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004). 
Even with large values of k in the approach of McCallum (2003) and Riezler and Vasserman (2004) (e.g. , k = 1,000), the approach we describe is likely to be at least as efficient as these alternative approaches. 
L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g. , Goodman 2003; Riezler and Vasserman 2004). 
