As we would like to avoid this problem, we use the smoothed sentence-level Bleu score as suggested in (Lin and Och, 2004). 
4.2 Impact of Paraphrases on Machine Translation Evaluation The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores (Papineni et al. , 2002; Koehn, 2004; Lin and Och, 2004; Stent et al. , 2005). 
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). 
Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amigo et al., 2005). 
As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. 
