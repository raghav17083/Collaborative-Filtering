Many previous QA systems (Echihabi et al. , 2003; Ravichandran, et al. , 2003; Ittycheriah and Roukos, 2002; Ittycheriah, 2001; Xu et al. , 2002) have well studied the features in the surface texts. 
The model we use is similar to (Shen et al. , 2005; Ravichandran et al. , 2003), which regard answer extraction as a ranking problem instead of a classification problem. 
(Ravichandran et al. , 2003) compared two maximum entropy-based QA systems, which view the AE as a classification problem and a re-ranking problem respectively, based on the word frequency features, expected answer class features, question word absent features and word match features. 
3.2 Maximum Entropy Ranking We view the problem as an instance of statistical ranking, a general machine learning paradigm used for example in statistical parsing (Collins, 2000) and question answering (Ravichandran et al. , 2003).3 The problem is to select, given a set of a0 possible candidates a1a3a2a5a4a7a6a7a8a9a8a9a8a9a6a10a2a12a11a14a13 (in our case, potential A speakers), the one candidate a2a16a15 that maximizes a given conditional probability distribution. 
Polynomial kernel: (, ) ( 1) p ij i j k =+xx xx Gaussian RBF kernel: 2 2 2 (, ) ij ij ke  = xx xx 5 Textual Features Since the features extracted from the surface texts have been well explored by many QA systems (Echihabi et al. , 2003; Ravichandran, et al. , 2003; Ittycheriah and Roukos, 2002; Ittycheriah, 2001; Xu et al. , 2002), we will not focus on the textual feature generation in this paper. 
