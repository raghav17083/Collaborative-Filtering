The resulting SCFG is identical to that used in Levy (2008b) and is given in Table 2.


In this case, the surprisal of a word (logP(wi|wi10 )) is equivalent to the Kullback-Leibler divergence of the beliefs after wi0 from the beliefs after wi10 (Levy, 2008a).


Such an analogy has been used to motivate surprisal-based theories of sentence processing (Hale, 2001; Levy, 2008a), where beliefs about the structure of a sentence after seeing the first i 1 words in the sentence, which we denote as wi10 , are updated upon encountering wi.


Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009).


