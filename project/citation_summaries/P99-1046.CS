These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; Tur et al. , 2001) exponential models (Beeferman et al. , 1999) or other probabilistic models (Hajime et al. , 1998; Reynar, 1999). 
Reynar (1999) describes a maximum entropy model that combines hand selected features, including: broadcast news domain cues, number of content word bigrams, number of named entities, number of content words that are WordNet synonyms in the left and right regions, percentage of content words in the right segment that are rst uses, whether pronouns occur in the rst ve words, and whether a word frequency based algorithm predicts a boundary. 
Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al. , 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al. , 1999). 
(1998) and Reynar (1999), we predict that keyword-based techniques for automatic argumentative zoning will not work well (cf. 
(Hearst, 1994) uses a measure of lexical cohesion between adjoining paragraphs in text; (Reynar, 1999) and (Beeferman et al. , 1999) combine a variety of features such as statistical language modelling, cue phrases, discourse information and the presence of pronouns or named entities to segment broadcast news; (Maskey and Hirschberg, 2003) use entirely non-lexical features. 
