The information content of this set is defined as mutual information I(F(w)) (Church and Hanks, 1990). 
Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words. 
Church and Hanks 1990; Smadja and McKeown 1990). 
Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al. , 1995; Luk, 1995; D. Lin, 1998a). 
In the field of statistical analysis of natural language data, it is common to use measures of lexical association, such as the informationtheoretic measure of mutual information, to extract useful relationships between words (e.g. Church and Hanks (1990)). 
