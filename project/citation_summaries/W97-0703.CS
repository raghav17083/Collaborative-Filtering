W03-0503:1	7:199	A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al. , 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al. , 2000).
---------------------------------------------------
P06-2119:2	27:141	Around the same time, Barzilay and Elhadad (1997) realized a non-greedy lexical chain, which determined the word sense after processing of all words, in the context of text summarization.
---------------------------------------------------
C04-1143:3	59:140	Previous related work on extractive systems included the use of semantic tagging and coreference/lexical chains (Saggion et al. , 2003; Barzilay and Elhadad, 1997; Azzam et al. , 1998), lexical occurrence/structural statistics (Mathis et al. , 1973), discourse structure (Marcu, 1998), cue phrases (Luhn, 1958; Paice, 1990; Rau et al. , 1994), positional indicators (Edmunson, 1964) and other extraction methods (Kuipec et al. , 1995).
---------------------------------------------------
W00-1438:4	127:135	In their research, Barzilay and Elhadad showed that lexieal chains could be an effective tool for automatic text summarization (Barzilay and EIhadad, 1997).
---------------------------------------------------
W00-1438:5	96:135	2.6 Comparison with Previous Work As mentioned above, this research is based on the work of Barzilay and Elhadad (1997) on lexical chains.
---------------------------------------------------
W00-1438:6	56:135	Table 1 denotes sample metrics tuned to simulate the system devised by Barzilay and Elhadad (1997).
---------------------------------------------------
W00-1438:7	34:135	Segment Segment 2.1 Overview Same 1 1 1 1 Our research on lexical chains as an intermediate Synonym 1 1 0 "O " representation for automatic text summarization folHypernym I 1 0 0 lows the research of Barzilay and Elhadad (1997).
---------------------------------------------------
W00-1438:8	131:135	,usable currenlfly, the-system provides a platform for generation research on automatic text summarization by providing an intermediate representation which has been shown to capture important concepts from the source text (Barzilay and Elhadad, 1997).
---------------------------------------------------
W00-1438:9	31:135	Barzilay and Elhadad (1997) dealt with some of tile limitations in Hirst and St-Onge's algorithm by examining every possible lexical chain which could be computed, not just those possible at a given point in the text.
---------------------------------------------------
N06-2004:10	3:158	1 Introduction Estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications: word-sense disambiguation (Banerjee and Pedersen, 2003), story segmentation (Stokes et al. , 2004), error correction (Hirst and Budanitsky, 2005), summarization (Barzilay and Elhadad, 1997; Gurevych and Strube, 2004).
---------------------------------------------------
P08-1041:11	61:250	The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad, 1997).
---------------------------------------------------
W00-0401:12	138:202	We have chosen Microsoft'97 Summarizer because, even if it only produces extracts, it was the only summarizer available in order to carry out this evaluation and because it has already been used in other evaluations (Marcu, 1997; Barzilay and Elhadad, 1997).
---------------------------------------------------
W09-2504:13	7:246	Inference using WordNet typically involves lexical substitutions for words in text based on WordNet relations, a process known as lexical chains (Barzilay and Elhadad, 1997; Moldovan and Novischi, 2002).
---------------------------------------------------
P98-1112:14	22:196	For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus on multiword noun phrases.
---------------------------------------------------
P98-2189:15	30:124	Such an extension may improve on the assessment of textual saliency and connectivity thus providing better generic summaries, as argued in Barzilay and Elhadad (1997).
---------------------------------------------------
P98-2189:16	115:124	Possible improvements of this approach can be implemented taking into account additional ways of assessing lexical cohesion such as:  the presence of synonyms or hyponyms across text units (Hoey, 1991; Hirst and StOnge, 1997; Barzilay and Elhadad 1997); 1162  the presence of lexical cohesion established with reference to lexical databases offering a semantic classification of words other than synonyms, hyponyms and subject domain codes;  the presence of near-synonymous words across text units established by using a method for estimating the degree of semantic similarity between word pairs such as the one proposed by Resnik (1995);  the presence of anaphoric links across text units (Hoey, 1991; Boguraev & Kennedy, 1997), and  the presence of formatting commands as indicators of the relevance of particular types of text fragments.
---------------------------------------------------
P98-2189:17	36:124	Secondly, the use of thesauri envisaged in both Hirst and St-Onge (1997) and Barzilay and Elhadad (1997) does not address the question of topical aptness.
---------------------------------------------------
P98-2189:18	29:124	A typical example is the use of thesaurus functions such as synonymy and hyponymy to extend the notion of word sharing across text units, as exemplified in Hirst and StOnge (1997) and Barzilay and Elhadad (1997) with reference to WordNet (Miller et al. , 1990).
---------------------------------------------------
C08-1009:19	6:223	Examples include summarization (Barzilay and Elhadad, 1997), question answering (Ramakrishnan et al., 2003) and machine translation (Chan and Ng, 2007).
---------------------------------------------------
C08-1124:20	58:230	Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries.
---------------------------------------------------
P05-2010:21	114:117	Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al. , 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998).
---------------------------------------------------
H05-1001:22	7:277	Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference(or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and Kennedy, 1999; Azzam et al. , 1999; Bergler et al. , 2003; Stuckardt, 2003) identify these terms by running a coreferenceor anaphoric resolver over the text.1 We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms.
---------------------------------------------------
W08-2206:23	7:193	59 60 Cramer 1 Introduction Various applications in Natural Language Processing, such as Question Answering (Novischi and Moldovan, 2006), Topic Detection (Carthy, 2004), and Text Summarization (Barzilay and Elhadad, 1997), rely on semantic relatedness (similarity or distance)1 measures either based on word nets and/or corpus statistics as a resource.
---------------------------------------------------
P01-1008:24	55:220	In some applications, only synonyms are considered as paraphrases (Langkilde and Knight, 1998); in others, looser definitions are used (Barzilay and Elhadad, 1997).
---------------------------------------------------
J02-4004:25	96:218	To select these, we use the idea of strong chains introduced by Barzilay and Elhadad (1997).
---------------------------------------------------
J02-4004:26	16:218	First, previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated, but with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997).
---------------------------------------------------
J02-4004:27	12:218	In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document.
---------------------------------------------------
J02-4004:28	14:218	Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process.
---------------------------------------------------
C00-2104:29	62:269	Ill a way, our idea is the mirror image of Barzilay and Elhadad (1997), who used Wordnet to identify lexical chains that would coincide with cohesive text segments.
---------------------------------------------------
P07-3015:30	93:170	For each top scored chain, Barzilay and Elhadad (1997) extract econometrics statistsical methods economic analysis case studies methods measurement evaluation statistical data data analysis cartography data collection surveys censures Figure 2.
---------------------------------------------------
P07-3015:31	22:170	The algorithm was reimplemented as soon as digital WordNet and Rogets became available (Barzilay and Elhadad, 1997) and its complexity was improved (Silber and McCoy, 2002; Galley and McKeown, 2003).
---------------------------------------------------
P07-3015:32	89:170	We have observed significant differences between reported functions on our data and achieved best results with the formula produced by Barzilay and Elhadad (1997): (2)      LCt LCt tfreq tfreq LC LCScore )() )( || 1()( Here, |LC| is the length of the chain and freq(t) is the frequency of the term t in the document.
---------------------------------------------------
J02-4005:33	466:526	Since then several methods and theories have been applied, including the use of term frequency  inverse document frequency (TF  IDF) measures, sentence position, and cue and title words (Luhn 1958; Edmundson 1969; Kupiec, Pedersen, and Chen 1995; Brandow, Mitze, and Rau 1995); partial understanding using conceptual structures (DeJong 1982; Tait 1982); bottom-up understanding, top-down parsing, and automatic linguistic acquisition (Rau, Jacobs, and Zernik 1989); recognition of thematic text structures (Hahn 1990); cohesive properties of texts (Benbrahim and Ahmad 1995; Barzilay and Elhadad 1997); and rhetorical structure theory (Ono, Sumita, and Miike 1994; Marcu 1997).
---------------------------------------------------
P06-3007:34	54:221	Barzilay and Elhadad (1997) segment the original text and construct lexical chains.
---------------------------------------------------
W98-1124:35	107:265	1998; Barzilay and Elhadad, 1997).
---------------------------------------------------
H01-1030:36	231:242	[9] R. Barzilay, M. Elhadad, Using Lexical Chains for Text Summarization, In Proceedings of the Intelligent Scalable Text Summarization Workshop (ISTS97), ACL, Madrid, 1997.
---------------------------------------------------
I08-2101:37	35:143	The understanding can be induced using dependencies between words (Barzilay and Elhadad, 1997), rhetorical relations (Paice and Johns, 1993), events (Filatova and Hatzivassiloglou, 2004).
---------------------------------------------------
I08-1015:38	24:177	Lexical chains have been used as an intermediate representation of text for various tasks such as au111 tomatic text summarisation (Barzilay and Elhadad, 1997; Silber and McCoy, 2002), malapropism detection and correction (Hirst and St-Onge, 1997), and hypertext construction (Green, 1998).
---------------------------------------------------
I08-1015:39	55:177	Prominent among them are those by (Hirst and St-Onge, 1997; Barzilay and Elhadad, 1997; Silber and McCoy, 2002; Jarmasz and Szpakowicz, 2003).
---------------------------------------------------
E09-1086:40	94:224	Chain-based classifier Various methods for building lexical chains have been proposed in the literature (Hirst and St-Onge, 1998; Barzilay and Elhadad, 1997; Silber and McCoy, 2002) but the basic idea is as follows: the content words of the text are considered in sequence and for each word it is determined whether it is similar enough to (the words in) one of the existing chains to be placed in that chain, if not it is placed in a chain of its own.
---------------------------------------------------
J05-3002:41	87:557	The first two of these scores are produced by Simfinder, and the salience score is computed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) as described below.
---------------------------------------------------
J05-3002:42	89:557	Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chainssequences of semantically related wordsare tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining which sentences are important for single-document summarization (Barzilay and Elhadad 1997; Silber and McCoy 2002).
---------------------------------------------------
C04-1057:43	29:156	Conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (Barzilay and Elhadad, 1997) or syntactic constraints between representations of concepts (Hatzivassiloglou et al. , 2001).
---------------------------------------------------
W09-2807:44	54:117	Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between list or elaboration relations and also in content selection for summarization (Barzilay and Elhadad, 1997).
---------------------------------------------------
P08-1116:45	49:268	Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997).
---------------------------------------------------
J98-3005:46	96:614	Alternatives to the use 473 Computational Linguistics Volume 24, Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997).
---------------------------------------------------
W03-1610:47	9:186	In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997).
---------------------------------------------------
W07-2106:48	12:78	Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions (Barzilay and Elhadad, 1997).
---------------------------------------------------
W07-2106:49	24:78	This intermediate representation of text has been used in many natural language processing applications, including automatic summarization (Barzilay and Elhadad, 1997; Silber and McCoy, 2003), information retrieval (Al-Halimi and Kazman, 1998), and intelligent spell checking (Hirst and St-Onge, 1998).
---------------------------------------------------
W07-2106:50	27:78	Later 476 research (Barzilay and Elhadad, 1997) significantly alleviated this problem at the cost of a worse running time (quadratic); computational inefficiency is due to their processing of many possible combinations of word senses in the text in order to decide which assignment is the most likely.
---------------------------------------------------
W07-2106:51	75:78	This method (summing edge weights in selecting the right sense) of WSD before constructing the chain (Gallery and McKeown, 2003) outperforms the earlier methods of Barzilay and Elhadad (1997) and Silber and McCoy (2003) but this method is highly dependent on the lexical cohesion among words in a context.
---------------------------------------------------
W00-0405:52	34:197	to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al. , 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al. , 1998).
---------------------------------------------------
W00-0405:53	10:197	Many of these documents are likely to repeat much the same information, while differing in certain i Most of these were based on statistical techniques applied to various document entities; examples include frait, 1983; Kupiec et al. , 1995; Paice, 1990, Klavans and Shaw, 1995; MeKeown et al. , 1995; Shaw, 1995; Aon et al. , 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al. , 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mortbn, 1998; Radev and McKeown, 1998; Strzalkowski et al. , 1998).
---------------------------------------------------
J02-1002:54	15:283	Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996).
---------------------------------------------------
W06-0205:55	22:256	As a consequence, Silber and McCoy (2002) propose a linear time version of (Barzilay and Elhadad, 1997) lexical chaining algorithm.
---------------------------------------------------
W06-0205:56	9:256	In particular, they have successfully been used in the field of Automatic Text Summarization(BarzilayandElhadad, 1997).
---------------------------------------------------
W06-0205:57	256:256	In particular, in future work, we want to compare our methodology using WordNet as the basic knowledge base, implement different similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodorow, 1998), experiment different Lexical Chains algorithms (Hirst and St-Onge, 1997; BarzilayandElhadad, 1997; GalleyandMcKeown, 2003), scale our greedy algorithm for realworld applications following (Silber and McCoy, 2002) ideas and finally evaluate our system in independent Natural Language Processing applications such as Text Summarization (Doran et al. , 2004).
---------------------------------------------------
W06-0205:58	219:256	Like in (Barzilay and Elhadad, 1997), we define a chain score which is defined in Equation 16 where |chain| is the number of words in the chain.
---------------------------------------------------
W06-0205:59	10:256	However, until now, Lexical Chaining algorithms have only been proposed for English as they rely on linguistic resources such as Thesauri (Morris and Hirst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003).
---------------------------------------------------
W06-0205:60	168:256	In this section, we propose a new greedy algorithm which can be seen as an extension of (Hirst and StOnge, 1997) and (Barzilay and Elhadad, 1997) algorithms as it allows polysemous words to belong to different chains thus breaking the one-word/oneconcept per document paradigm (Gale et al. , 1992).
---------------------------------------------------
W06-0205:61	25:256	Their evaluation shows that their algorithm is more accurate than (Barzilay and Elhadad, 1997) and (Silber and McCoy, 2002) ones.
---------------------------------------------------
W06-0205:62	17:256	But, asBarzilayandElhadad(1997) point at, the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet.
---------------------------------------------------
W06-0205:63	18:256	So, Barzilay and Elhadad (1997) propose the first dynamic method to compute Lexical Chains.
---------------------------------------------------
W06-0205:64	240:256	5.2 Qualitative Evaluation Inthissection, asitisdonein(BarzilayandElhadad, 1997; Teich and Fankhauser, 2004), we present the c=5 c=6 c=7 c=8 Doc 1 19 13 7 7 Doc 2 13 6 3 3 Doc 3 3 4 4 4 Doc 4 6 4 3 3 Table 3: # Clusters per Lexical Chain fivehighest-scoringchainsforthebestthresholdthat we experimentally evaluated to be c = 7 for each domain (See Tables 4, 5, 6, 7).
---------------------------------------------------
W06-0205:65	228:256	So, in this section, we will only present some results generated by our architecture (like (Barzilay and Elhadad, 1997; Teich and Fankhauser, 2004) do), althoughweacknowledgethatothercomparativeevaluations (with WordNet, with Human Lexical Chains or within independent applications like Text Summarization) must be done in order to draw definitive conclusions.
---------------------------------------------------
W06-0205:66	36:256	For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the one-word/one-concept per document paradigm (Gale et al. , 1992)1.
---------------------------------------------------
W06-0205:67	174:256	4.1 Algorithm Our chaining algorithm is based on both approaches of (Barzilay and Elhadad, 1997) and (Hirst and StOnge, 1997).
---------------------------------------------------
J02-4001:68	43:155	Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measured by the number of shared words, synonyms, or anaphora (e.g. , Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999).
---------------------------------------------------
J02-4001:69	136:155	They present a linear time algorithm to extract lexical chains from a source document (the lexical-chain approach was originally developed by Barzilay and Elhadad [1997] but used an exponential time algorithm).
---------------------------------------------------
W04-1017:70	46:202	Other systems exploit the co-occurrence of particular concepts (Barzilay and Elhadad, 1997; Lin and Hovy, 2000) or syntactic constraints between concepts (McKeown et al. , 1999).
---------------------------------------------------
W99-0211:71	22:174	Barzilay and Elhadad (1997) describe a technique for text summarization based on lexical chains.
---------------------------------------------------
W07-0203:72	42:307	Examples of such relations are word cooccurrence (e.g. , Salton et al. , 1997), synonyms and antonyms (e.g. , Barzilay and Elhadad, 1997), logical relations, such as concordance or contradiction, and syntactic relations.
---------------------------------------------------
W07-0203:73	96:307	Concerning (a), synonyms, antonyms, and term repetition were all considered, as suggested by others (e.g. , Barzilay and Elhadad, 1997).
---------------------------------------------------
W07-0203:74	98:307	Finally, in tackling (c) and, thus, grading the importance of the relations for sentence similarity, we adopted the same weights proposed by Barzilay and Elhadad (1997) in their lexical chaining method, which is discussed in more detail below.
---------------------------------------------------
W07-0203:75	30:307	Some of them correspond to full summarization methods by themselves: Lexical Chaining (Barzilay and Elhadad, 1997), Relationship Mapping (Salton et al. , 1997), and Importance of Topics (Larocca Neto et al. , 2000).
---------------------------------------------------
W07-0203:76	115:307	Both methods and parameters are mapped onto the feature space and are defined as follows: (M) Lexical Chaining (Barzilay and Elhadad, 1997).
---------------------------------------------------
W07-0203:77	45:307	The discourse structure is intended to help retrieving, e.g., the main topics of the document (e.g, Barzilay and Elhadad, 1997; Larocca Neto et al. , 2000) or its rhetorical structure (e.g. , Marcu, 1999), in order to provide the means for AS.
---------------------------------------------------
P99-1071:78	44:283	A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et al. , 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadad, 1997).
---------------------------------------------------
H05-1122:79	6:224	1 Introduction Ever since Morris and Hirst (1991)s groundbreaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993).
---------------------------------------------------
W06-3408:80	55:145	Topic segmentation algorithms generally rely on a lexical cohesion signal that requires smoothing in order to eliminate noise from changes of word choices in adjoining statements that do not indicate topic shifts (Hearst, 1997; Barzilay and Elhadad, 1997).
---------------------------------------------------
