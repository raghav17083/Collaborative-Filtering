While many NLG researchers have attempted to transfer the ideas of Centering Theory to generation (Not, 1996; Yeh and Mellish, 1997; McCoy and Strube, 1999; Henschel et al. , 2000; Kibble and Power, 2000), there has yet been no substantial return contribution to the field of anaphora resolution. 
For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system's performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through "task efficacy" measures (Young 1997). 
Finally, there are significant obstacles to verifying the correctness of existing pronominalization algorithms for any pronominalization theory (Not, 1996; Yeh and Mellish, 1997; McCoy and Strube, 1999; Henschel et al. , 2000; Kibble and Power, 2000): the lack of natural language generation systems that can produce large enough texts to bring discourse-level processes into play. 
As regards evaluation, NLG systems have been evaluated e.g. by using human judges to assess the quality of the texts produced (Coch, 1996; Lester and Porter, 1997; Harvey and Carberry, 1998); by comparing the systems performance to that of humans (Yeh and Mellish, 1997); or through task efficacy measures, i.e., measuring how well the users so, and the distribution of topics and of evaluations is too broad to be telling. 
Theory evaluations are typically done by comparing predictions of a theory to what is observed in a humanauthored corpus (for example, (Yeh and Mellish, 1997)). 
