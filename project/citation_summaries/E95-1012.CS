Allowing the values to be partially ambiguous depending on a context raises questions what is the accurate level of disambiguation (Wolinski and Przepirkowski, 2001) and how to model it probabilistically in terms of random variables taking disjunctive values (Brew, 1995).


For example, the context-free description can be addressed with solutions borrowed from work in learning PCFGs (Jelinek et al, 1992), and the distribution can be estimated by training on sample data (Eisele, 1994 and Brew, 1995).


All approaches have in common that they try to model a probability distribution over the readings of the UBG, which can be used to rank the competing analyses of a given sentence; see, e.g., Briscoe and Carroll (1993), Eisele (1994), Brew (1995), Abney (1997), Goodman (1997), Bod and Kaplan (1998), Johnson et al.


1 Brew (1995) sketches a probabilistic version of Head-Driven Phrase Structure Grammar (HPSG).


However, it differs from recent proposals by, for example, Brew (1995), to associate probabilities with values on paths in a TFS formalism underlying HPSG, as the probabilistic information is much less fine-grained.


