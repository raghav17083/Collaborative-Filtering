The method for blending the distributions applies equally well to smoothing the transition probabilities pij, i.e., the tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal.


272-274) and (Krenn and Samuelsson 1996, pp.


The lexical probabilities depend on the identity of the current word for known words and on a suffix tree smoothed with successive abstraction (Samuelsson 1996) for guessing the tags of unknown words.


There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church & Gale, 1991), and Back-off type methods (Bahl et al. , 1983; Katz, 1987; Chen & Goodman, 1996; Samuelsson, 1996).


An alternative method to determine the interpolation weights without iterative training on held-out data is given in Samuelsson (1996).


