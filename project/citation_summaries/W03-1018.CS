Kazama and Tsujii (2003) describe a method for training a L1-regularized log-linear model with a bound constrained version of the BFGS algorithm (Nocedal, 1980). 
For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). 
Following the terminology of Dudk and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are lscript1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. 
For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). 
In (Chen, 2009), we show that for many types of exponential language models, if a training and test set are drawn from the same distribution, we have Htest  Htrain + D Fsummationdisplay i=1 |i| (2) where Htest denotes test set cross-entropy; Htrain denotes training set cross-entropy; D is the number of events in the training data; the i are regularized parameter estimates; and  is a constant independent of domain, training set size, and model type.1 This relationship is strongest if the  = {i} are estimated using lscript1+lscript22 regularization (Kazama and Tsujii, 2003). 
