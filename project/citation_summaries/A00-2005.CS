Combine the outputs of the several systems: previously studied techniques include: i) voting techniques (van Halteren et al. , 1998; Tjong Kim Sang, 2000; Henderson and Brill, 1999; Henderson and Brill, 2000), ii) switching among several systems according to confidence values they provide (Henderson and Brill, 1999), iii) stacking techniques (Wolpert, 1992) which train a second stage classifier for Association for Computational Linguistics. 
Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al. 1998). 
3 SSVM Ensemble for Semantic Parsing Although the bagging and boosting techniques have known to be effective for improving the performance of syntactic parsing (Henderson and Brill, 2000), in this section we focus on our ensemble learning of SSVM for semantic parsing and propose a new effective switching model for either bagging or boosting model. 
Previously studied techniques for collecting such systems include: i) using several existing real systems (van Halteren et al. , 1998; Brill and Wu, 1998; Henderson and Brill, 1999; Tjong Kim Sang, 2000), ii) bagging/boosting techniques (Henderson and Brill, 1999; Henderson and Brill, 2000), and iii) switching the data expression and obtaining several models (Tjong Kim Sang, 2000). 
1 Introduction In the recent corpus-based NLP research, system combination techniques have been successfully applied to several tasks such as parts-of-speech tagging (van Halteren et al. , 1998), base noun phrase chunking (Tjong Kim Sang, 2000), and parsing (Henderson and Brill, 1999; Henderson and Brill, 2000). 
