Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n2) times of training/test routines, it cannot handle a set that consists of hundreds of templates. 
Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). 
Ding and Chang (2008) focused on argument classification for Chinese verbal predicates with hierarchical feature selection strategy. 
As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied. 
2 Related Work Previous work on Chinese SRL mainly focused on how to implement SRL methods which are successful on English, such as (Sun and Jurafsky, 2004; Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2008). 
