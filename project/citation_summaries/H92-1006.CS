The speech understanding community is quite active in refining evaluation measures and in developing new ones (see e.g., Price et al. 1992, Hirschman et al. 1993, Dahl et al. 1994). 
For example, a dialog system can be evaluated by measuring the system's ability to help users achieve their goals, the system's robustness in detecting and recovering from errors of speech recognition or of understanding, and the overall quality of the system's interactions with users (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Polifroni et al. , 1992; Price et al. , 1992; Simpson and Fraser, 1993). 
Another problem is that dialog evaluation is not reducible to transcript evaluation, or to comparison with a wizard's reference answers (Bates and Ayuso, 1993; Polifroni et al. , 1992; Price et al. , 1992), because the set of potentially acceptable dialogs can be very large. 
Subjective metrics that have been used include (Danieli and Gerbino, 1995; Hirschman and Pao, 1993; Simpson and Fraser, 1993; Danieli et al. , 1992; Bernsen, Dybkjaer, and Dybkjaer, 1996) :  Implicitrecovery (IR): the system's ability to use dialog context to recover from errors of partial recognition or understanding. 
The experiment was designed to make it possible to apply the PARADISE evaluation framework (Walker et al. , 2000), which integrates and unifies previous approaches to evaluation (Price et al. , 1992; Hirschman, 2000). 
