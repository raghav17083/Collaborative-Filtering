To address this problem, Grishman and Sterling (1994) proposed a method of smoothing conditional probabilities using the probability values of similar words, where the similarity between words is judged based on co-occurrence data (see also Dagan, Marcus, and Makovitch \[1992\] and Dagan, Pereira, and Lee \[1994\]). 
Following Dagan, Pereira, and Lee (1994), we modify Katz's formulation by writing Pr(w2\]wl) instead P(w2), enabling us to use similarity-based estimates for unseen word pairs instead of basing the estimate for the pair on unigram frequency P(w2). 
Successful approaches aimed at trying to overcome the sparse data limitation include backoff (Katz 1987), Turing-Good variants (Good 1953; Church and Gale 1991), interpolation (Jelinek 1985), deleted estimation (Jelinek 1985; Church and Gale 1991), similarity-based models (Dagan, Pereira, and Lee 1994; Essen and Steinbiss 1992), Pos-language models (Derouault and Merialdo 1986) and decision tree models (Bahl et al. 1989; Black, Garside, and Leech 1993; Magerman 1994). 
Besides hard algorithms there have also been studies to soft clustering (Pereira, Tishby, and Lee, 1993, Dagan, Pereira, and Lee, 1994) where the distribution of every word is smoothed with the nearest k words rather than placed in a class which supposedly has a uniform behavior. 
We compared several such methods, including that of Dagan, Pereira, and Lee (1994) and the cooccurrence smoothing method of Essen and Steinbiss (1992), against classical estimation methods, including that of Katz, in a decision task involving unseen pairs of direct objects and verbs, where unigram frequency was eliminated from being a factor. 
