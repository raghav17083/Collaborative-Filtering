In this work, the task is to select facial displays for an animated talking head to use while presenting output in the COMIC multimodal dialogue system (Foster et al. , 2005), which generates spoken descriptions and comparisons of bathroom-tile options.


(2007), [e]valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited. Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety.


2 Corpus-based generation of facial displays for an ECA The experiments in this paper make use of the output components of the COMIC multimodal dialogue system (Foster et al., 2005), which adds a multimodal talking-head interface to a CAD-style system for redesigning bathrooms.


In the basic COMIC process for generating multimodal output (Foster et al. , 2005), facial displays are selected using simple rules based only on the pitch accents specified by the text generation system.


2 Corpus collection and annotation1 The recording scripts for the corpus were created by the output planner of the COMIC multimodal dialogue system (Foster et al. , 2005) and consisted of a total of 444 sentences describing and comparing various tile-design options.


