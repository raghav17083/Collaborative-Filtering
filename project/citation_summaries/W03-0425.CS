N09-1032:1	14:250	Supervised learning methods can effectively solve NER problem by learning a model from manually labeled data (Borthwick, 1999; Sang and Meulder, 2003; Gao et al., 2005; Florian et al., 2003).
---------------------------------------------------
W06-1670:2	15:362	80s (Carreras et al. , 2002; Florian et al. , 2003), while Bio-NER accuracy ranges between the low 70s and 80s, depending on the data-set used for training/evaluation (Dingare et al. , 2005).
---------------------------------------------------
W03-1026:3	185:193	We also applied the classifier combination technique discussed in this paper to English and German (Florian et al. , 2003b).
---------------------------------------------------
P05-1001:4	197:242	Previous best results: FIJZ03 (Florian et al. , 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al. , 2003).
---------------------------------------------------
P08-1076:5	198:228	test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data (Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data (Florian et al., 2003) 93.87 88.76 own large gazetteers, 2M-word labeled data (Suzuki et al., 2007) N/A 88.41 27M-word unlabeled data [sup.
---------------------------------------------------
D08-1072:6	197:207	Some empirical work has considered adding versus multiplying classifier output (Tax et al., 2000), using local accuracy estimates for combination (Woods et al., 1997), and applications to NLP tasks (Florian et al., 2003).
---------------------------------------------------
W06-1660:7	10:202	The relevant algorithms include Maximum Entropy (Borthwick, 1999; Klein et al. , 2003), Hidden Markov Model (HMM) (Bikel et al. , 1999; Klein et al. , 2003), AdaBoost (Carreras et al. , 2003), Memorybased learning (Meulder and Daelemans, 2003), Support Vector Machine (Isozaki and Kazawa, 2002), Robust Risk Minimization (RRM) Classification method (Florian et al. , 2003), etc. For Chinese NER, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics.
---------------------------------------------------
N06-1012:8	10:293	1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al. , 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al. , 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al. , 2004).
---------------------------------------------------
W05-0611:9	28:171	Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al. , 2002), as well as voting over sets of different classifiers (Florian et al. , 2003).
---------------------------------------------------
D07-1083:10	174:316	Moreover, the experimental results shown in Tables 3 and F=1 additional resources ASO-semi 89.31 unlabeled data (27M words) (Ando and Zhang, 2005) (Florian et al. , 2003) 88.76 their own large gazetteers, 2M-word labeled data (Chieu and Ng, 2003) 88.31 their own large gazetteers, very elaborated features HySOL 88.14 unlabeled data (17M words) supplied gazetters HySOL 87.20 unlabeled data (17M words) Table 5: Previous top systems in NER (CoNLL2003) experiments F=1 additional resources ASO-semi 94.39 unlabeled data (Ando and Zhang, 2005) (15M words: WSJ) HySOL 94.30 unlabeled data (17M words: Reuters) (Zhang et al. , 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91  Table 6: Previous top systems in Chunking (CoNLL-2000) experiments 4 indicate that HySOL is rather robust with respect to the hyper-parameter since we can obtain fairly good performance without a prior distribution.
---------------------------------------------------
N06-1010:11	14:171	They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al. , 2003; Klein et al. , 2003; Finkel et al. , 2005).
---------------------------------------------------
W03-0419:12	115:153	The inclusion of extra named entity recognition systems seems to have worked well (Florian et al. , 2003).
---------------------------------------------------
W03-0419:13	137:153	A majority vote of five systems (Chieu and Ng, 2003; Florian et al. , 2003; Klein et al. , 2003; McCallum and Li, 2003; Whitelaw and Patrick, 2003) performed best on the English development data.
---------------------------------------------------
W03-0419:14	109:153	One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al. , 2003).
---------------------------------------------------
W03-0419:15	85:153	Transformation-based learning (Florian et al. , 2003), Support Vector Machines (Mayfield et al. , 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each.
---------------------------------------------------
W03-0419:16	142:153	The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization (Florian et al. , 2003).
---------------------------------------------------
W03-0419:17	71:153	Two more systems used them in combination with other techniques (Florian et al. , 2003; Klein et al. , 2003).
---------------------------------------------------
W03-0419:18	73:153	Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al. , 2003; Klein et al. , 2003; Mayfield et al. , 2003; Whitelaw and Patrick, 2003).
---------------------------------------------------
W03-0434:19	87:105	See (Florian et al. , 2003) for such a study.
---------------------------------------------------
W09-1119:20	158:223	It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003).
---------------------------------------------------
P06-2060:21	26:122	2 Combining Classifiers Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al. , 2000), named entity extraction (Florian et al. , 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories.
---------------------------------------------------
P09-1116:22	190:254	3.1) 83.78 W500 88.34 +4.56 P64 9.73 +5.94 P125 89.80 +6.02 W500 + P125 90.62 +6.84 W500 + P64 0.63 +6.85 W500 + P125 + P64 90.90 +7.12 W500 + P125 + P64+pos 90.62 +6.84 LDC64 87.24 +3.46 LDC125 8.33 +4.55 LDC64 +LDC125 88.44 +4.66 (Suzuki and Isozaki, 2008) 89.92 (Ando and Zhang, 2005) 89.31 (Florian et al., 2003) 88.76 (Chieu and Ng, 2003) 88.31 (Klein et al., 2003) 86.31 Table 5 Example queries and their classes ford field    Sports/American Football    Information/Local & Regional    Sports/Schedules & Tickets john deere gator    Living/Landscaping & Gardening    Living/Tools & Hardware    Information/Companies & Industries    Shopping/Stores & Products    Shopping/Buying Guides & Researching justin timberlake lyrics    Entertainment/Music    Information/Arts & Humanities    Entertainment/Celebrities Table 6 Labeler Consistency  L1  L2 L3 Average F1 0.538 0.477 0.512 0.509 P 0.501 0.613 0.463 0.526 1035 Given an input x, represented as a vector of m features: (x1, x2, , xm), a logistic regression classifier with parameter vector L(w1, w2, , wm) computes the posterior probability of the output y, which is either 1 or -1, as L:U
---------------------------------------------------
W05-0610:23	117:275	Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al. , 2003), the SVM-based system (May eld et al. , 2003) and the Perceptron-based system (Carreras et al. , 2003).
---------------------------------------------------
