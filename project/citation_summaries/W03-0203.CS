Few attempts had been made before (Mitkov and Ha, 2003), in which a semi-automatic question generation on students knowledge of linguistic terms are evaluated. 
The only previous use of post-edit evaluation in NLG that we are aware of is Mitkov and An Ha [2003], but their evaluation is relatively small, and they give little information about it. 
Mitkov et al.(2003) generated questions for a linguistics exam in a semi-automatic way and evaluated that it exceeds manually made ones in cost and is at least equivalent in quality. 
The cost-effectiveness of post-edit evaluation is less clear if the evaluators must organize and pay for the post-editing, as Mitkov and An Ha [2003] did. 
Mitkov and Ha (2003) established a system which generates reading comprehension tests in a semi-automatic way by using an NLP-based approach to extract key concepts of sentences and obtain semantically alternative terms from WordNet. 
