D09-1119:1	64:258	SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006).
---------------------------------------------------
D09-1119:2	19:258	(Kudo and Matsumoto, 2000; Nivre et al., 2006)).
---------------------------------------------------
D09-1119:3	102:258	This setting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use.
---------------------------------------------------
P05-1072:4	175:208	Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software.
---------------------------------------------------
P05-1072:5	30:208	sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system.
---------------------------------------------------
N04-1032:6	38:287	(2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.
---------------------------------------------------
W05-0634:7	29:111	TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system.
---------------------------------------------------
P07-1029:8	42:237	Further details can be found in Kudo and Matsumoto (2000; 2003).
---------------------------------------------------
P07-1029:9	22:237	Kudo and Matsumoto (2000) used SVM as a classification engine and achieved an FScore of 93.79 on the shared task NPs.
---------------------------------------------------
P07-1029:10	83:237	These are the same settings as in (Kudo and Matsumoto, 2000; Goldberg et al. , 2006).
---------------------------------------------------
P07-1029:11	11:237	In (Goldberg et al. , 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well.
---------------------------------------------------
P02-1063:12	8:161	SVMs have good generalization performance and can handle a large number of features, and are applied to some tasks  Presently with Oki Electric Industry successfully (Joachims, 1998; Kudoh and Matsumoto, 2000).
---------------------------------------------------
P06-1087:13	115:204	Some settings (Kudo and Matsumoto, 2000) also include the IOB tags of the two previously tagged tokens as features (see Fig.
---------------------------------------------------
P06-1087:14	25:204	Kudo and Matsumoto (2000) used an SVM based algorithm, and achieved NP chunking results of 93.72% precision, 94.02% recall and 93.87 F-measure for the same shared task data, using only the words and their PoS tags.
---------------------------------------------------
P06-1087:15	31:204	They used the same features as Kudo and Matsumoto (2000), and achieved over-all chunking performance of 92.06% precision, 92.09% recall and 92.08 F-measure (The results for NP chunks alone were not reported).
---------------------------------------------------
P06-1087:16	111:204	SVMs learn binary classifiers, but the method can be extended to multiclass classification (Allwein et al. , 2000; Kudo and Matsumoto, 2000).
---------------------------------------------------
P06-1087:17	112:204	SVMs have been successfully applied to many NLP tasks since (Joachims, 1998), and specifically for base phrase chunking (Kudo and Matsumoto, 2000; 2003).
---------------------------------------------------
N01-1025:18	19:201	In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achieved higher accuracy than previous approaches.(Joachims, 1998; Taira and Haruno, 1999; Kudo and Matsumoto, 2000a).
---------------------------------------------------
N01-1025:19	98:201	In using DP matching, we limit a number of ambiguities by applying beam search with width a142 . In CoNLL 2000 shared task, the number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto, 2000a).
---------------------------------------------------
N01-1025:20	185:201	In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5.
---------------------------------------------------
W02-0301:21	30:341	Even with a more feasible method, pairwise (Kreel, 1998), which is employed in (Kudo and Matsumoto, 2000), we cannot train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation.
---------------------------------------------------
W02-0301:22	69:341	A polynomial function defined as (sxi x j + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al. , 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty.
---------------------------------------------------
W02-0301:23	46:341	Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al. , 2000; Kudo and Matsumoto, 2001).
---------------------------------------------------
W02-0301:24	23:341	Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al. , 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al. , 2001; Ratnaparkhi, 1996).
---------------------------------------------------
W02-0301:25	54:341	Number of glucocorticoid receptors O O B-PROTEIN I-PROTEIN in lymphocytes and  O B-CELLTYPE O  3.2 Support Vector Machines Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al. , 2001).
---------------------------------------------------
P06-2076:26	42:210	This is because support vector machines are comparatively better than other methods in many research areas (Kudoh and Matsumoto, 2000; Taira and Haruno, 2001; Small Margin Large Margin Figure 4: Maximizing margin Murata et al. , 2002).
---------------------------------------------------
P06-2076:27	61:210	Data consisting of more than two categories is generally handled using the pair-wise method (Kudoh and Matsumoto, 2000).
---------------------------------------------------
P06-1145:28	191:230	This type of sequential tagging method regard as a chunking procedure (Kudo and Matsumoto, 2000) at sentence level.
---------------------------------------------------
P08-4003:29	28:54	Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnsons reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors.
---------------------------------------------------
P06-1054:30	73:237	We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel.
---------------------------------------------------
W00-0726:31	124:150	Kudoh and Matsumoto (2000) created 231 support vector machine classifiers to predict the unique pairs of chunk tags.
---------------------------------------------------
H05-1059:32	153:175	Method Recall Precision F-score SVM (Kudoh and Matsumoto, 2000) 93.51 93.45 93.48 SVM voting (Kudo and Matsumoto, 2001) 93.92 93.89 93.91 Regularized Winnow (with basic features) (Zhang et al. , 2002) 93.60 93.54 93.57 Perceptron (Carreras and Marquez, 2003) 93.29 94.19 93.74 Easiest-first (IOB2, second-order) 93.59 93.68 93.63 Full Bidirectional (Start/End, first-order) 93.70 93.65 93.70 Table 6: Chunking F-scores on the test set (Section 20 of the WSJ, 2012 sentences).
---------------------------------------------------
W01-0502:33	38:239	In (Kudoh and Matsumoto, 2000) the methods of all pairs was used to learn phrase annotations for shallow parsing.
---------------------------------------------------
N04-4036:34	41:121	We use TinySVM2 along with YamCha3 (Kudo and Matsumoto (2000, 2001)) as the SVM training and test software.
---------------------------------------------------
N04-1030:35	70:243	For our experiments, we used TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and test software.
---------------------------------------------------
I08-5008:36	18:195	His system is an extension of Kudos chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.
---------------------------------------------------
N07-1070:37	41:232	We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software.
---------------------------------------------------
P02-1064:38	22:168	SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al. , 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a).
---------------------------------------------------
D09-1153:39	188:244	6.1.2 Classifier For both syntactic and semantic chunking, we used TinySVM along with YamCha7 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001).
---------------------------------------------------
C02-1020:40	11:141	Therefore, SVMs have been recently used for the natural language processing such as text categorization (Joachims, 1998; Taira and Haruno, 1999), chunk identification (Kudo and Matsumoto, 2000b), dependency structure analysis (Kudo and Matsumoto, 2000a).
---------------------------------------------------
P01-1069:41	149:185	For example, the previous best result in the literature was achieved by a combination of 231 kernel support vector machines (Kudoh and Matsumoto, 2000) with an overall a132a76a133 a110a76a37 value of a209 a204 a41a206a73a165 . Each kernel support vector machine is computationally significantly more expensive than a corresponding Winnow classifier, and they use an order of magnitude more classifiers.
---------------------------------------------------
P01-1069:42	124:185	Similar methods have also appeared in other natural language processing systems (for example, in (Kudoh and Matsumoto, 2000)).
---------------------------------------------------
W01-0712:43	159:210	The best result at the workshop was obtained with Support Vector Machines (Kudoh and Matsumoto, 2000).
---------------------------------------------------
N06-1025:44	66:213	The features used in our model are all binary-valued feature functions (or indicator functions), e.g. fI SEMROLE(ARG0/RUN, COREF) =      1 if candidate pair is coreferent and antecedent is the semantic argument ARG0 of predicate run 0 else In our system, a set of pre-processing components including a POS tagger (Gimenez & M`arquez, 2004), NP chunker (Kudoh & Matsumoto, 2000) and the Alias-I LingPipe Named Entity Recognizer2 is applied to the text in order to identify the noun phrases, which are further taken as referring expressions (REs) to be used for instance generation.
---------------------------------------------------
P06-2013:45	60:216	Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000).
---------------------------------------------------
