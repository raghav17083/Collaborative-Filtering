In some cases, the resulting corpus provides training data for spoken language systems (Hirschman et al. 1993), is used as a target for improved systems (Moore, Lemaire, and Rosenblum 1996), or forms a test set for evaluating the performance of an existing natural language system (Whittaker and Stenton 1989; Hirschman et al. 1993). 
Examples include the use of decision trees for syntactic analysis (Magerman, 1995), coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995), and cue phrase identification (Litman, 1994); the use of inductive logic programming for learning semantic grammars and building prolog parsers 113 (Zelle and Mooney, 1994; Zelle and Mooney, 1993); the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b), and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al. , submitted). 
One might be tempted to argue that recent evaluation efforts within the field of information extraction (IE) systems (Chinchor et al. , 1993) are going to remedy this shortcoming. 
For these experiments, we drew training and test cases (241 instances) from MUC-3 texts that describe Latin American terrorist events (Chinchor et al. , 1993). 
Case-based learning algorithms have been used in NLP for context-sensitive parsing (Simmons and Yu, 1992), for text categorization (Riloffand Lehnert, 1994); for lexical tagging tasks like part-of-speech tagging and semantic feature tagging (Daelemans et al. , submitted, Cardie, 1994, Cardie, 1993a); for semantic interpretation (e.g. , concept extraction (Cardie, 1994, Cardie, 1993a)); and for a number of low-level language acquisition tasks, including stress acquisition (Daelemans et al. , 1994) and graphemeto-phoneme conversion (Bosch and Daelemans, 1993). 
