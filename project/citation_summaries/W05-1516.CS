In which case, the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005; Wang et al. , 2006). 
For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al. , 2005), and yet they are not being used in current large margin training algorithms. 
Finally, we compared our results to the probabilistic parsing approach of (Wang et al. , 2005), which on this data obtained accuracies of 0.7631 on the CTB test set and 0.6104 on the development set. 
In which case, the parsing problem reduces to a19a38a37 a3a40a39a42a41a44a43a46a45a47a39a26a48 a49a51a50a42a52a54a53a56a55a58a57 a59 a53a61a60a63a62a6a64a65a60a67a66a44a57a68a50a29a49 sa4a6a5 a20 a21a69a5 a24 a17 (1) where the score sa4a6a5a27a20a70a21 a5a25a24a71a17 can depend on any measurable property of a5a30a20 and a5a25a24 within the tree a19 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005). 
For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al. , 2005), and yet they are not being used in current large margin training algorithms. 
