To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and 3-CEAF (Luo, 2005). 
5.2 Coreference Results For evaluating the coreference performance, we rely on three primary metrics: (i) the link based MUC metric (Vilain et al., 1995), the mention based B3 metric (Bagga and Baldwin, 1998), and the entity based CEAF metric (Luo, 2005). 
The alignment between any two coreference labelings, say C1 and G, for a document is the best one-to-one map (Luo, 2005) between the entities of C1 and G. To align the entities of C1 with those of G, under the assumption that an entity in C1 may be aligned with at most only one entity in G and vice versa, we need to generate a bipartite graph between the entities of C1 and G. Now the alignment task is a maximum bipartite matching problem. 
To score the output of the coreference models, we employ the commonly-used MUC scoring program (Vilain et al., 1995) and the recently-developed CEAF scoring program (Luo, 2005). 
There does not appear to be a single standard eval5http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC-6 (Vilain et al., 1995), CEAF (Luo, 2005), and B-CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. 
