1.3 Our Approach In past work (Heeman and Allen, 1997; Heeman, 1998), we introduced an alternative formulation for using POS tags in a language model. 
In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): PPL = 2 En (4) En  1 N N X i=1 log 2 ^ P(w i jw i1 i2 )  1 N N X i=1 log 2 P t i2;i ^ P(w i t i jw i1 i2 t i1 i2 ) ^ P(w i1 i2 t i1 i2 ) P t i2;i1 ^ P(w i1 i2 t i1 i2 ) Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). 
Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al. , 2004). 
Heemans (1998) POS LM achieves a perplexity reduction compared to a trigram LM by instead rede ning the speech recognition problem as determining: W ;T = arg max W;T P(W;TjA) = arg max W;T P(W;T)P(AjW;T) arg max W;T P(W;T)P(AjW) where T is the POS sequence t N 1 associated with the word sequence W = w N 1 given the speech utterance A.TheLMP(W;T)isajoint probabilistic model that accounts for both the sequence of words w N 1 and their tag assignments t N 1 by estimating the joint probabilities of words and tags: P(w N 1 ;t N 1 )= N Y i=1 P(w i ;t i jw i1 1 ;t i1 1 )(2) Johnson (2001) and La erty et al. 
In Heemans POS language model (Heeman, 1998), the joint probability of word sequence and associated POS sequence was estimated directly, which has been demonstrated to be superior to the conditional probability previously used in the class-based models (Johnson, 2001). 
