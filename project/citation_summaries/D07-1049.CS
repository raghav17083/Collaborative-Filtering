Figure 2 Input Types Language models are abstracted to enable different implementations to be used and provide a framework for more complex models such as factored LM and the Bloom filter language model (Talbot and Osborne 2007). 
3 Related Work 3.1 Randomised Language Models Talbot and Osborne (2007) used a Bloom filter (Bloom, 1970) to encode a smoothed LM. 
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. 
3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a no-false-negative guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and Osborne, 2007a). 
In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that 1For extensions of the framework presented here to standalone smoothed Bloom filter language models, we refer the reader to a companion paper (Talbot and Osborne, 2007). 
