6 Adding Context-Level Features Research in other domains (Litman et al. , 2001; Batliner et al. , 2003) has shown that features representing the dialogue context can sometimes improve the accuracy of predicting negative user states, compared to the use of features computed from only the turn to be predicted. 
We are currently exploring the use of other emotion annotation schemas for emotion prediction, such as those that incorporate categorizations encompassing multiple dimensions (Craggs, 2004; Cowie et al. , 2001) and those that examine emotions at smaller units of granularity than turns (Batliner et al. , 2003). 
Systems such as CIRCSIM (Evens and Michael 2006), BEETLE (Zinn et al. 2002), the Geometry Explanation Tutor (Aleven et al. 2003), Why2/Atlas (VanLehn et al. 2002), ITSpoke (Litman et al. 2006), SCOT (PonBarry et al. 2006), ProPL (Lane and VanLehn 2005) and AutoTutor (Graesser et al. 2003) support research that has begun to the see the emergence of a core set of foundational requirements for mixedinitiative natural language interaction that occurs in the kind of tutorial dialogue investigated here. 
Lexical information has been shown to improve speech-based emotion prediction in other domains (Litman et al. , 2001; Lee et al. , 2002; Ang et al. , 2002; Batliner et al. , 2003; Devillers et al. , 2003; Shafran et al. , 2003), so our rst non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). 
As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (Litman et al. , 2001; Lee et al. , 2001; Ang et al. , 2002; Lee et al. , 2002; Batliner et al. , 2003; Devillers et al. , 2003; Shafran et al. , 2003), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time. 
