The only corpus that is annotated with a variety of clinical named entities is the CLEF project (Roberts et al., 2007) . Most of the works mentioned above are annotated on formal clinical reports and scientific literature abstracts, which generally conform to grammatical conventions of structure and readability. 
The detailed description of the task, and the challenge itself can be found in (Pestian et al., 2007) and online3. 
The accuracy for cmc2007 is significantly better than the results reported in Patrick et al. 
The radiology report corpus that was used for the clinical coding challenge (Pestian et al., 2007) organised by the Computational Medicine Center in Cincinatti, Ohio in 2007 was annotated for negations and uncertainty along with the scopes of each phenomenon. 
Our finding that token unigram features are capable of solving the task accurately agrees with the the results of previous works on hedge classification ((Light et al., 2004), (Med287 lock and Briscoe, 2007)), and we argue that 2-3 word-long phrases also play an important role as hedge cues and as non-speculative uses of an otherwise speculative keyword as well (i.e. to resolve an ambiguity). 
