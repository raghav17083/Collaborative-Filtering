In other types of spoken dialogue systems, the users subjective judgments about using the system are often considered a primary system performance metric; e.g., user satisfaction has been measured via surveys which ask users to rate systems during use along dimensions such as task ease, speech input/output quality, user expectations and expertise, and user future use (Mcurrency1oller, 2005b; Walker et al. , 2002; Bonneau-Maynard et al. , 2000; Walker et al. , 2000; Shriberg et al. , 1992). 
User satisfaction ratings (Kamm, 1995; Shriberg, Wade, and Price, 1992; Polifroni et al. , 1992) have been frequently used in the literature as an external indicator of the usability of a dialogue agent. 
The results can be disastrous for applications, since when a recognizer misrecognizes, a common reaction is to hyperarticulate (Shriberg et al 1992). 
Subjects, Scenarios, Instructions Data collection will proceed as described in Shriberg et al. 1992 \[16\] with the following exceptions: (1) updated versions of the SRI Template Matcher and recognizer will be used; (2) subjects will use a new data collection facility (the room is smaller and has no window but is acoustically similar to the room used previously); (3) the scenarios to be solved have unique solutions; (4) the debriefing questionnalre will be a merged version of the questions used on debriefing questionnaires at SRI and at MIT in separate experiments; and (5) each subject will solve two scenarios, one using the SRI SLS and one using the SRI/MIT hybrid SLS. 
Evaluation methods independent of dialogue strategy have focused on measuring the extent to which systems for interactive problem solving aid users via log-file evaluations (Polifroni et al. , 1992), quantifying repair attempts via turn correction ratio, tracking user detection and correction of system errors (Hirschman and Pao, 1993), and considering transaction success (Shriberg et al. , 1992). 
