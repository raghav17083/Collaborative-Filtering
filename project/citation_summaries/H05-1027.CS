For this reason, a new class of language model, the discriminative language model, has been proposed recently to augment generative language models (Gao et al., 2005; Roark et al., 2007). 
The average parameters are defined as )./()()( 11, MT T t M i it davgd =  ==  (9) 3.4 The minimum sample risk method The minimum sample risk (MSR, Gao et al. , 2005) training algorithm is motivated by analogy with the feature selection procedure for the boosting algorithm (Freund et al. , 1998). 
Discriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect (Gao et al. , 2005; Roark et al. , 2007), and these models can handle both non-local and overlapping information. 
The four models we compare are a maximum a posteriori (MAP) method and three discriminative training methods, namely the boosting algorithm (Collins, 2000), the average perceptron (Collins, 2002) and the minimum sample risk method (Gao et al. , 2005). 
Recently, it has been shown that a linear model estimated using discriminative training methods, such as the boosting and perceptron algorithms, outperforms significantly a traditional word trigram model trained using maximum likelihood estimation (MLE) on several tasks such as speech recognition and Asian language text input (Bacchiani et al. 2004; Roark et al. 2004; Gao et al. 2005; Suzuki and Gao 2005). 
