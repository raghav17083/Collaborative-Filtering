The probability P(W) is given by the language model and can be decomposed as: P(W) = Y i P(wijw1;i1) (2) Figure 2: Example of Word-based Channel Model The distribution P(W) can be defined using n-grams, structured language model (Chelba, 1997), or any other tool in the statistical language modeling.


It is likely that more sophisticated language models, such as structure models (Chelba, 1997; Chelba and Jelinek, 1998), or longer ngram models would lead to the system generating headlines that were more similar in phrasing to real headlines because longer range dependencies shelf Carnegie Mellon University summarizer, which was the top ranked extraction based summarizer for news stories at the 1998 DARPA-TIPSTER evaluation workshop (Tip, 1998).


