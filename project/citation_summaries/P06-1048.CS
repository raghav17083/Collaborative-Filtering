1 Introduction Whilethereareafewnotableexceptions(Horiand Furui, 2004; Yamagata et al., 2006), it would be safe to say that much of prior research on sentence compression has been focusing on what we might call model-intensive approaches, where the goal is to mimic human created compressions as faithfully as possible, using probabilistic and/or machine learning techniques (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Cohn and Lapata, 2007; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 
According to Clarke & Lapata (2006), this measure reliably correlates with human judgements. 
An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). 
Compression rate can be indirectly manipulated by adoptinglossfunctionsthatencourageordiscourage compression (see Figure 4), but admittedly in other frameworks (e.g. , Clarke and Lapata (2006a)) the length of the compression can be influenced more naturally. 
We also present results on Clarke and Lapatas (2006a) Broadcast News corpus.4 This corpus was created manually (annotators were asked to produce compressions for 50 Broadcast news stories) and poses more of a challenge than Ziff-Davis. 
